\documentclass{article}
\usepackage[margin = 1in]{geometry}
\usepackage[table]{xcolor}
\usepackage{amsmath,amsthm,amsfonts,dsfont,mathtools,float,setspace, booktabs, algorithm,tcolorbox,xparse,colortbl,longtable,array,multirow,wrapfig,float,pdflscape,tabu, setspace, threeparttable,threeparttablex,makecell,natbib, pdfpages}
\usepackage[normalem]{ulem}
\usepackage[english]{babel}
\bibliographystyle{plainnat}

\DeclarePairedDelimiter{\ceil}{\lceil}{\rceil}
\DeclarePairedDelimiter\floor{\lfloor}{\rfloor}
\newtheorem{theorem}{Theorem}
\newtheorem{lemma}[theorem]{Lemma}
\newtheorem{refproof}{Proof}
\author{Adam Elder}
\newcommand{\bs}{\boldsymbol}
\newcommand{\sh}{\textcolor{red}}
\newcommand{\pr}{\text{Pr}}
\newcommand{\vmat}{\Sigma}
\newcommand{\indep}{\raisebox{0.05em}{\rotatebox[origin=c]{90}{$\models$}}}
\usepackage[mathscr]{euscript}
\usepackage{algorithm}
\usepackage[noend]{algpseudocode}
\definecolor{Gray}{gray}{0.95}
\newcommand{\rvo}{X}
\newcommand{\norm}{f}
\newcommand{\rvoo}{x}
\newcommand{\disto}{P}
\newcommand{\tst}{\hat{\boldsymbol{t}}}
\newcommand{\rvt}{Y}
\newcommand{\rvv}{Z}
\newcommand{\distv}{Q}
\newcommand{\Gammaf}{\Gamma_{\Sigma}}
\newcommand{\Gammafe}{\Gamma_{\hat{\Sigma}}}
\newcommand{\pf}{\Phi}
\newcommand{\gi}{\Upsilon}

\begin{document} 
\doublespacing


\begin{abstract}
It is common across scientific disciplines to be interested in predicting an outcome using some (potentially large) number of covariates.  A first step to constructing accurate predictions is to decide if any covariate is associated with the outcome. Testing for the existence of an association can be done with a variety of existing methods, but frequently a trade off is made between generalizability (ability to use the test for different parameters of interest and data generating mechanisms) and power (ability to detect an association when one exists).  We propose an approach testing for the existence of an association between an outcome and any number of covariates that can be carried out for most parameters of interest that performs nearly as well as tests designed for a specific parameter and data generating mechanism.  We compare our method with other modern methods in a simple simulated data setting.  We then study the tests performance in a setting with a complex sampling scheme, and a setting with a complex parameter of interest.  
\end{abstract}

\section{Introduction}

One of the central goals of science is to be able to predict some outcome, often with the help of other related information.  This goal is often attained using the construction of mathematical and statistical models in which many covariates are used to predict the outcome of interest. Because the accuracy of the models is dependent on the presence of an association between the outcome of interest and some number of the covariates it is important to know if any associations exist before attempting to create a model. While this first step is always important, it has become more important as it becomes more common for large numbers of covariates to be collected, often without an explicit outcome of interest in mind.

To test if any covariate is associated with the outcome, a standard approach is to test each covariates association with the outcome. It is desirable for the testing procedure to account for the potentially large number of hypotheses that are simultaneously being tested when using this approach.

Work with \textcolor{red}{simultaneous hypothesis testing}
began with Tukey in 1953 \citep{miller_simultaneous_1981}.  Previous work by Bonferroni was used by \citep{dunn_estimation_1959,dunn_multiple_1961} to come up with some of the first multiple hypothesis testing procedures.  Further improvements were proposed by \citep{hochberg_sharper_1988,holm_simple_1979,s._holland_improved_1988}.  Bonferroni-based correction procedures are easy to apply to already existing tests, while guaranteeing family-wise error control. However because these tests ignore the joint distribution of the test statistics, they suffer from low power, especially in cases where the probability of rejecting each hypothesis is highly correlated. 

Newer procedures \citep{donoho_higher_2004} and \sh{add other papers here} attain improved power compared to Bonferroni-based methods, but often rely on asymptotics to obtain these results, and don't account for the irregularity of the estimator on which their test is based.  Subsequent work \citep{mckeague_adaptive_2015, pan_powerful_2014, xu_adaptive_2016} addressed these concerns by accounting for the adaptive nature of the considered tests.  While these tests obtain higher power while maintaining type one error control, they were constructed using assumptions about the data generating mechanism, and for only a single measure of association between the outcome of interest and each covariate.  As a result, investigators may still choose to use bonferroni correction based tests if they are interested in a different measure of association or are unsure of how deviations from the assumed data generating mechanism could effect the properties of the testing procedure. 

In this article a testing procedure is proposed that can be used for a wide variety of data generating mechanisms and parameters of interest, but also achieves comparable power to tailor made procedures. Section \ref{sec:Working Examples} describes the data generating mechanisms that are considered to evaluate the performance of the test and the competing test made for the given data generating mechanism.  Section \ref{sec:prop_test_proc} proposes the testing procedure. Section \ref{sec:sim_stdy} shows the performance of our testing procedure in the three considered simulation settings. Section \ref{sec:data_app} shows the application of our method to real world data.  Section \ref{sec:discuss} summarizes the findings of this article and notes potential weaknesses and extensions of our procedure.
%This is done using a two step procedure.  In the first step, the limiting distribution of the entire vector of parameter estimates is estimated.  Next, an adaptive procedure selects a transformation of the vector of parameter estimates and distribution which is guessed provides the best power.  P-values are obtained using a permutation procedure. 

\section{Working Examples}
\label{sec:Working Examples}
Let $\rvo_1, \dots, \rvo_n$ be independent identically distributed draws from some distribution $\disto$, and let $\boldsymbol{\rvo} = \{\rvo_1, \dots, \rvo_n\}$ . Let $\rvo_i = \left(Y_i, W_{1 i}, \dots, W_{d i}\right), i \in \{1, \dots n\}$ where $Y$ is the outcome of interest, and each $W$ is a covariate. Let $\psi_1 = \Psi_1(\disto), \dots, \psi_a = \Psi_a(\disto)$ be measures of association between $Y$ and some combination of the $W_j$'s.  While the results found in this article are valid for any integer $a$, for the remainder of this article assume $a = d$, the number of covariates. Also, let $\psi_j$ correspond to a measure of association between $Y$ and $W_j$.  The null hypothesis for our test will be the strong null: 

\begin{align*}
H_0: \psi_1 = \psi_2 = \dots = \psi_a = 0 \hspace{0.1cm}\text{  versus  } \hspace{0.1cm} H_1: \psi_j \neq 0 \text{ for some } j \in \{1, \dots, a\}.
\end{align*}

Last, let $\mathscr{M}$ denote the set of all possible distributions, and let $\mathscr{M}_0  \subset \mathscr{M}$ be the subset of distributions in $\mathscr{M}$ satisfying $H_0$.

We consider three different simulated data settings to study the test's performance. 

\subsection{Correlation Parameter}
We will compare our method to a Bonferroni correct marginal testing method and the method described in \citep{zhang_comment_2015}.  The settings considered will be the same as the firs setting in \citep{mckeague_adaptive_2015}.  The parameter of interest, $\psi_j(P)$ will be the correlation between the outcome of interest and the $j$'th covariate. 

The vector of covariates in this setting will be generated from a normal distribution with mean zero and a variance covariance of $\Sigma$ with $\Sigma_{ij}$ equal to $\rho$ when $i \neq j$ and equal to 1 when $i = j$. Three different models for the outcome of interest ($Y$) will be considered. For all considered settings, $\varepsilon \sim N(0, 1)$ and is independent of all $X$. In the first model $Y = \varepsilon$, in the second model $Y = X_1 / 4 + \varepsilon$, and in the third model $Y = \sum_{k = 1}^{10} \beta_k X_k + \varepsilon$ where $\beta_k = 0.15$ for $k = \{1, \dots, 5\}$, and $\beta_k = -0.1$ for $k = \{6 \dots 10\}$.  Sample sizes of $100$ and $200$, dimensions of $10$, $50$, $100$, $150$, and $200$, and $\rho$ of  $0, 0.5$, and $0.8$ are considered.  All combinations of model, sample size, dimension and $\rho$ are considered, and every test's performance is measured for all considered settings.

\subsection{Missing Data Example}
In the second example, $Y$ is binary, $\Delta$ is a missingness indicator, and each $W_j$ is a covariate of interest.  When $\Delta = 0$ we don't observe $Y$.  The identifying assumption is $\Delta \indep Y | W$, and the parameter of interest is the risk ratio under a poission working model for the probability that $Y = 1$.

\begin{align*}
	\Psi_{j}\left(P^{\text{full}}\right) &= \frac{\text{Cov}\left(\log\left(Pr \left(Y = 1 |  W_j\right)\right), W_j\right)}{\text{Var}(W_j)}.\\
\end{align*}

Using the identifying assumption, the observed data parameter is:

\begin{align*}
	\tilde{\Psi}_{j}\left(P^{\text{obs}}\right)&= \frac{\text{Cov}\left(\log\left(E\left[Pr \left(\Delta  Y = 1 | \Delta = 1, W = W\right) |  W_j\right]\right), W_j\right)}{\text{Var}(W_j)}\\
\end{align*}

The data are drawn from a binomial model:

\begin{align*}
    \log(Pr(Y = 1 | W)) = \beta_0 + W^\top \beta.
\end{align*}

There will be three settings considered which determine  the values of the $\beta$'s in the data generating model:

In all simulation settings the working model is used to take draws from $y$. In each setting, the vector $\boldsymbol{W}$ is draw from a multivariate normal with mean zero and covariance matrix $\Sigma_{DE2}$ where $\Sigma_{DE2, i, j} = 1$ for $i = j$ and $0.6$ for $i \neq j$.  $A$ is drawn from a binomial distribution independent from $W$.  For all three settings 

\begin{align*}
	\text{logit}\left(Pr(Y = 1 | w, a)\right) = \sum_{i = 1}^{d} \beta_{i} w_i
\end{align*}

In the first setting $\beta_1 \dots \beta_d = 0$.  In the second setting $\beta_1 = 3$ and $\beta_2, \dots \beta_d = 0$.  In the last setting, $\beta_1 \dots \beta_5 = 1$, $\beta_6 \dots \beta_{10} = -1$ and $\beta_{11} \dots \beta_d = 0$.  Data are generated from all three models with every possible combination of sample size ($n = 100$, or $200$), and dimension ($d = 10, 50, 100, 150$, or $200$).

\subsection{Marginal Structural Model}
The third data example is a marginal structural model in which we test if the average treatment effect of a binary treatment $A$ is modified by any covariates $W_j$.  The marginal structural model for each $W_j$ is defined by the working model:

\begin{align*}
\text{logit}\left(Pr(Y^{(a)} = 1 | w)\right) = \beta_0 + \beta_1 a + \beta_2 w_j + \beta_3 w_j a
\end{align*}

in which our parameter of interest is:

\begin{align*}
(\beta_0^*, \beta_1^*, \beta_2^*, \beta_3^*) = \text{argmin}_{\beta_0, \beta_1, \beta_2, \beta_3}\int\left(\text{logit}\left(Pr(Y^{(a)} = 1 | w\right) - (\beta_0 + \beta_1 a + \beta_2w_j + \beta_3 w_j a) \right)^2 dP(w, a)
\end{align*}

The parameter of interest is $\beta_3^*$, but is worth noting that parameter is different from a usual logistic regression parameter because we are marginalizing over all $w_i$, not just $w_j$.

In all simulation settings the working model is used to take draws from $y$. In each setting, the vector $\boldsymbol{W}$ is draw from a multivariate normal with mean zero and covariance matrix $\Sigma_{DE3}$ where $\Sigma_{DE3, i, j} = 1$ for $i = j$ and $0.6$ for $i \neq j$.  $A$ is drawn from a binomial distribution independent from $W$.  For all three settings 

\begin{align*}
	\text{logit}\left(Pr(Y = 1 | w, a)\right) = \beta_1 a + \sum_{i = 1}^{d} \beta_{i + 1} w_i + \sum_{j = 1}^d \gamma_{j} w_j a
\end{align*}

In every setting, $\beta_1 = 0.2$, $\beta_2, \dots \beta_{d/2 + 1} = 2/\sqrt{d}$, and $\beta_{d/2 + 1} \dots \beta_{d + 1} = 0$.  In the first (null) setting, $\gamma_{1} \dots \gamma{d} = 0$.  In the second setting, $\gamma_{1} = 3$ and $\gamma_{2} \dots \gamma_{d} = 0$.  In the final setting $\gamma_{1} \dots \beta_{5} = 1, \gamma_{6} \dots \beta_{10} = -1$, and $\gamma_{11} \dots \gamma_{d} = 0$.
Data are generated from all three models with every possible combination of sample size ($n = 100$, or $200$), and dimension ($d = 10, 50, 100, 150$, or $200$).


% \subsection{\sh{Something else possibly Tim's parameter (has something to do with survival)}}
% In this setting, the outcome is \sh{some measure of health}, and the covariates correspond to genetic measurements $W_{ij}$ that take value one for \sh{genes or snips which dont match the reference genome} and zero for those that do match. It is of interest if the measured outcome is associated with these genetic measurements.  

\section{Proposed Testing Procedure}
\label{sec:prop_test_proc}
For some test statistic $\tst$, any test of $H_0 : \disto \in \mathscr{M}_0$ versus $H_1 : \disto \not\in \mathscr{M}_0$ can be characterized by an acceptance region $\Theta_0(\disto) \subset \mathbb{R}^d$.  Letting $ \tst \xrightarrow{P_0} \rvv \sim \distv(\disto)$ for any $P_0 \in \mathscr{M}_0$, the region $\Theta_0(\disto)$ can be chosen so the probability of rejection under the null is controlled asymptotically:
\begin{align}
  Pr_{\distv}\{\rvv \not \in \Theta_0(P)\} = 1 - \alpha \text{ for every } P_0 \in \mathscr{M}_0.\label{eqn:alpha_rej}
\end{align}
While there are many regions satisfying \eqref{eqn:alpha_rej},  we focus for now on a particular class of regions defined using $\ell_p$ norms which will naturally lead to a straightforward testing procedure. For simplicity, first consider regions defined using an $\ell_2$ norm:
\begin{align}
	\Theta_0(r) = \left\{\omega : \|\omega\|_2 \leq r\right\}. \label{eqn:region}
\end{align}
A region satisfying \eqref{eqn:alpha_rej} and \eqref{eqn:region} has a radius defined by: 
% \textcolor{blue}{be consistent on indexing ``$pr$'' -- if you're going to write as $pr_{q(p)}$, do this throughout!}
\begin{align}
	r_\alpha(\disto) = \min\left\{r : Pr_{\distv}(\|\rvv\|_2 \leq r) \geq 1 - \alpha \right\}. \label{eqn:ra}
\end{align}
To construct a test using the region defined above, let $\hat{\boldsymbol{\Psi}} : \mathbb{R}^d \to \mathbb{R}$ be an estimator of $\boldsymbol{\psi}$, and let $\Hat{\boldsymbol{\psi}} \equiv \hat{\boldsymbol{\Psi}}(x)$ be an estimate of $\boldsymbol{\psi}$. Suppose for now that $\sqrt{n}\hat{\boldsymbol{\psi}}$ converges in law to a normal distribution $\distv(P)$ when $P$ is contained in the model space  $\mathcal{M}_0$. The test can be defined by 
\begin{align}
\label{eqn:simp_test}
	\text{reject } H_0 \text{ if } \|\sqrt{n} \hat{\boldsymbol{\psi}}\|_2 \geq r_\alpha(P_0),
\end{align} 
and the corresponding p-values are defined by $
	\text{Pr}_\distv(\|\rvv\|_2 \geq \|\sqrt{n} \hat{\boldsymbol{\psi}}\|_2)$.

% This example illustrates how functions mapping into $\mathbb{R}$ (the $\ell_2$ norm in this case) allow us to define a test and p-value for $H_0$.



\begin{figure}
	\centering
	\includegraphics[width = \linewidth]{figure_code/new_pwr_cmpr.jpeg}
	\caption{Plots of 100 observations from a limiting distribution of a hypothetical vector of parameter estimators in $\mathbb{R}^2$ (A) under the null, (B) under an alternative with $\psi_1 = 0, \psi_2 \neq 0$, and (C) under an alternative with $\psi_1, \psi_2 \neq 0$. The 95\% quantiles for the data based on the max (blue) and $\ell_2$ (red) norms under the null are given in all three panels. If a test statistic fell within the blue regions the test would fail to reject $H_0$ if the $\ell_\infty$ norm was used, but would reject $H_0$ if the $\ell_2$ norm was used.  The converse is true for the red regions.  Depending on the alternative, the $\ell_\infty$ norm (B) or the $\ell_2$ norm(C) will achieve higher power.}
	\label{fig:figure1}
\end{figure}

So far, we have outlined a method of constructing a test of $H_0$, but it is easy to imagine other tests defined using different norms. The natural next question is which test will perform the best, and does the choice of norm make a difference.  To explore this question, consider a simple example comparing the test described in equation \eqref{eqn:simp_test} with a test that is identical except it uses the $\ell_\infty$ norm (maximum absolute value) instead of the $\ell_2$ norm.  Figure \ref{fig:figure1}, panel (A) illustrates these two tests in $\mathbb{R}^2$. One hundred draws are taken from $Y$, a bivariate normal distribution with mean zero and identity covariance matrix. All observations in panel (A) except the five with the largest $\ell_2$ norm are contained within the red circle. The blue square contains all observations in panel (A) except the five with the largest $\ell_\infty$ norm.  The circle and square represent the acceptance regions of tests using empirical estimates of the $95^{\text{th}}$ percentile of $\ell_2(\rvt)$ and $\ell_\infty(\rvt)$ respectively corresponding to $r_\alpha(P)$ from \eqref{eqn:ra}.  The same square and circle are redrawn in panels (B) and (C) to illustrate the test's performance under two alternatives. Observations that fall within the blue shaded region would result in a rejected null hypothesis if the $\ell_2$ norm was used to define the test, but not if the $\ell_\infty$ norm was used. The converse is true of the red shaded region. Panels B shows draws from an alternative in which $\psi_2 \ne 0$ and $\psi_1 = 0$ and the $\ell_\infty$ norm performs better (achieves higher power). Panel C shows draws from an alternative in which $\psi_1$ and $\psi_2 \neq 0$ and the $\ell_2$ norm performs better. 

While both acceptance regions are created to achieve asymptotic type $1$ error control, depending on the alternative one test will outperform the other. Panel $B$ shows an alternative in which only $\psi_2$ is non-zero.  Because the max norm only considers the largest coordinate, shifting each observation in only a single direction will have larger impact on the max norm of the observations compared to the $\ell_2$ norm.  This trend is shown by the numerous red observations outside of the blue box (equivalent to rejecting $H_0$) and inside the red circle (equivalent to failing to reject $H_0$). In contrast, there is only a single observation that is outside the red circle, and inside the blue box. The converse trend is shown in panel $C$. Here, the $\ell_2$ norm performs better, because it takes into account both coordinates of the shift whereas the $\ell_\infty$ norm can only take into account one of these coordinate shift. 

The efficiency that can be gained from using the correct norm can be quite large, especially in high dimension settings.  Work done by \citep{pinelis_schur2-concavity_2014} states that even for dimensions as small as $3$ the gains in asymptotic efficiency for selecting the correct norm can become arbitrarily large between two potential norms \textcolor{blue}{Review this}.  

% For every distribution $\distv$ with support contained in $\mathbb{R}^d$, let $\Gamma_{\distv}$ be a function mapping from  $\mathbb{R}^d$ to $\mathbb{R}$. The distribution of $\Gamma_\distv(\rvv)$ (where $\rvv \sim \distv$) can be compared to $\Gamma_\distv(\sqrt{n}\hat{\boldsymbol{\psi}})$ to obtain p-values and define a test.  
% \textcolor{blue}{I see $\Gamma_{\distv}$ being an $\mathbb{R}^d$ to $\mathbb{R}$ function. I don't see $\Gamma_Q(x)$ being a function? I also don't understand how this maps ``from a ... distribution'' to anywhere? Do you mean: ``For any distribution $Q$ with support contained in $\mathbb{R}^d$, let $\Gamma_Q$ be a function mapping from $\mathbb{R}^d$ to $\mathbb{R}$''?}
% \textcolor{blue}{What is $\tilde\rvv \overset{d}{=} \rvv$ telling us? Don't we already know that the probabilities over both of these quantities are indexed by $Q$? Also, why use different notation at all? Why not just $\rvv^*$ for both? Here because $c_{0.8}$ is not random so this is distinction does not need to be made.}
% In this scenario, the test can be carried out by comparing our test statistic, $\Gamma_\vmat(\sqrt{n}\hat{\boldsymbol{\psi}}, \|\cdot\|)$ to $\Gamma_\vmat(\rvv, \|\cdot\|).$

% This test statistic was then compared to the estimated limiting distribution of the test statistic under the null.  The key components of this test were to create a real valued summary of the observed test statistic, and compare this test statistic to draws of the test statistic under the null.  In the previous example, the test statistic was the quantile of the $\ell_2$ norm of our vector of parameter estimates.  We already know the distribution of this statistic is uniform from zero to one under the null, so calculating the p-value is trivial.

% However, we could consider other test statistics.  One example would be the estimated power of an observation is from the $80\%$ quantile of the limiting distribution under the null.  Here, while smaller values indicate more evidence against the null, the distribution of this test statistic is not as straightforward.  While more difficult, the distribution of this test statistic can be be estimated using a bootstrap.  We can take a single observation from our estimated limiting distribution of $\sqrt{n}\hat{\psi}$ and then find its multiplicative distance from the $80\%$ quantile under the $\ell_2$ norm.  This can be done many times to create a limiting distribution of our test statistic.  A p-value can then be obtained by comparing the test statistic to the bootstrapped test statistics.	 

\subsection{Adaptive selection of a norm}
In the previous section we showed that a test can be defined with a summarizing function and norm. However, the choice of norm can influence the power of the test. In many scenarios it will not be clear a priori which test will have maximal power for the true alternative. The procedure proposed in this section adaptively selects a norm and it will be shown that this procedure achieves greater power than a test with a fixed norm, while maintaining type 1 error control.  

The test defined in \eqref{eqn:simp_test} is a function of three objects; the chosen norm, $\sqrt{n}\hat{\boldsymbol{\psi}}$, and the limiting distribution of  $\hat{t} = \sqrt{n}\hat{\boldsymbol{\psi}}$. Note that the limiting distribution of $\hat{t}$ under the null will always be a multivariate normal with mean zero with some covariance $\Sigma$. Thus potential tests can be defined using the finite dimensional $\Sigma$ matrix rather than the infinite dimension distribution $Q(P)$.

In order to choose between tests (or norms) it will be important to measures the tests performance in a way that is consistent across norms.  While all tests will reject as the test statistic becomes larger, if the measure of performance was the norm $\hat{t}$ then the optimal norm would always be the $\ell_1$ norm.  To achive this goal, we will consider other mappings from $\mathbb{R}$ into $\mathbb{R}$ that are comparable across norms.  Consider a general function $\Gammaf$, of a vector in $\mathbb{R}^d$ (corresponding to $\tst$). While $\Gammaf(x)$ should get larger as $x$ moves away from the null, $\Gammaf$ can be quite general.
An example of a more complicated $\Gammaf$ is given below:
\begin{align}
	\Gammaf(x) &= Pr_\distv(\|\rvt + x\| > c_{0.8})  \text{ where }  c_{0.8} \equiv \min_{c}\{c : Pr(\|\rvt\| < c) \geq 0.8 \} \text{ and } \rvt \sim N(0, \Sigma). \label{gamma:pow}
\end{align}
% To simplify this test, replace  $\sqrt{n}\hat{\boldsymbol{\psi}}$ with $ \tst \equiv \sqrt{n}\Sigma^{-1/2}\hat{\boldsymbol{\psi}}$. Because $\tst$ will always converge to a standard multivariate normal regardless of which $P \in \mathscr{M}_0$ is true \textcolor{blue}{(point to proof)}, for the test defined using $\tst$: 

% \begin{align}
% \label{eqn:simp_test_simp}
% 		\text{reject } H_0 \text{ if } \|\sqrt{n} \hat{\boldsymbol{\psi}}\|_2 \geq r_\alpha \text{ where } r_\alpha = \min\left\{r : Pr(\|\rvt\|_2 \leq r) \geq 1 - \alpha \right\} \text{ where } \rvt \sim N(\boldsymbol{0}, \Sigma).
% \end{align}

% has the property that $r_\alpha$ will not vary between null distributions. Using this simplification, the result of this test is determined by the norm chosen and the value of the test statistic.
% \subsection{\textcolor{blue}{Creating a rejection region using a function}}

%For the example of the $\ell_2$ norm, the p-value calculation was a function of the limiting distribution $Z$ and the test statistic $\sqrt{n}\hat{\boldsymbol{\psi}}$
% Because the limiting distribution of $\sqrt{n}\hat{\boldsymbol{\psi}}$ under $H_0$ is always a mean zero multivariate normal, we will index our general functions $\Gamma$ by the $d$ by $d$ variance covariance matrix instead of the infinite dimensional distribution function.
%   Under $H_0$, the limiting distribution of $\sqrt{n}\hat{\boldsymbol{\psi}}$ will be a mean zero multivariate normal distribution. 
% any test defined using an acceptance region can also be carried out using a function, say $\Gamma_\distv(x)$ that maps from a value and distribution in $\mathbb{R}^d$.  Trivially, this function could be defined as: $\Gamma_{\distv(\disto)}(x) = I\{x \in \Theta_0(\disto)\}$.  However, in other cases, (for example the $\ell_2$ ball), this function can be more intuitive and can change depending on the norm.  
% In the previous example, $\Gamma_\distv(x) = Pr_\distv\left(\|\rvv\| \geq \|x\| \right)$.  For this definition of $\Gamma$, we knew the distribution of $\Gamma_\distv(\rvv)$ is uniformly distributed on $(0, 1)$.  However, for other $\Gamma$, this may not be the case. Consider 

We can define the test using $\Gammaf$ as 
\begin{align*}
	\text{reject } H_0 \text{ if } \Gammaf(\tst) > c_{1 - \alpha} \text { where } c_{1 - \alpha} = \min_{c}\{c : \text{Pr}(\Gammaf(Y) \geq c) < \alpha\} \text{ where } Y \sim N(\boldsymbol{0}, \Sigma)
\end{align*}
% \textcolor{blue}{I don't understand the end of this sentence}. 
% While the the above test has nice properties (type one error control, consistency), it is not clear how this test will perform compared to other tests with similar properties.

Considering the $\Gammaf$ for which it is possible to compare across norms, we can now define a new, adaptive $\Gammaf^*$ which is the pointwise maximum of all of the considered $\Gammaf$'s. First define $\Gamma_{\Sigma, 1}(x), \dots, \Gamma_{\Sigma, p}(x)$ as collection of functions in which the functions only differ by the norm used in their definition. Next, define
% by defining a $\Gamma$ to those defined earlier, except the optimal norm is selected, and the corresponding summary value of $\Gamma$ with the optimal norm is returned.  For example, if we had $p$ different norms, $\norm_1, \dots, \norm_p$ and some summary function $\Gamma_\vmat(x, \|\cdot\|)$, we would define the adaptive version of $\Gamma$ as
\begin{align*}
	\Gammaf^*(x) = \max\left\{\Gamma_{\Sigma, 1}(x), \Gamma_{\Sigma, 2}(x), \dots, \Gamma_{\Sigma, p}(x)\right\}.
\end{align*}
\textcolor{blue}{This could be the min if smaller values indicate larger distances away from the norm (such as p-values).}  

While this function is more complicated than before, distribution of $\Gammaf^*(\rvt)$ can still be compared to $\Gammaf^*(\tst)$ to obtain a p-value. Also, while it may be difficult to obtain the exact distribution of $\Gammaf^*(\rvt)$, the distribution is a function of $\vmat$, so obtaining good approximations of $\Gammaf^*(\rvt)$ is possible by taking many draws from $\rvt$.

% Let $g_1, \dots, g_k$ be a list candidate norms, and let $\boldsymbol{Z}_1, \dots, \boldsymbol{Z}_B$ and $\boldsymbol{V}_1, \dots, \boldsymbol{V}_B$ independent draws from $Q$.  For each of the possible norms, let a p-value be defined using the following quantity:

% \begin{align*}
% s_{p}\left(\sqrt{n}\hat{\boldsymbol{\psi}}, \boldsymbol{Z}_1, \dots, \boldsymbol{Z}_B\right) = \frac{1}{B}\sum_{i=1}^B I\left\{g_p\left(\sqrt{n}\hat{\boldsymbol{\psi}}\right)  \leq g_p(\boldsymbol{Z}_i)\right\}
% \end{align*}

% Using the $c_{p, \alpha}$ defined earlier, it is possible to obtain estimates of power for each norm for any assumed vector of parameters:
% \begin{align*}
% \hat{\omega}_{\alpha, p}(\boldsymbol{h}) = \frac{1}{B} \sum_{i=1}^B I\left\{g_p\left(\boldsymbol{h} + \boldsymbol{V}_i\right) \geq \hat{c}_{\alpha, p}\right\}
% \end{align*}
% The last quantity considered is the multiplicative distance the vector of parameter estimates is from a vector in the same direction which obtains a certain power:
% \begin{align*}
% \hat{r}_{\alpha, p}(a) = \min\left\{r : \hat{\omega}_{\alpha, p}(r \times \sqrt{n}\hat{\boldsymbol{\psi}}) > a\right\}
% \end{align*}
% As described above, there are a multitude of ways to evaluate the performance of the various norms.  Let $f_p(x_1, \dots, x_k)$ be the function that takes the observed data and a norm and returns a measure of the norm's estimated performance given the observed data. $f$ could be $s_p$, $\hat{\omega}_{\alpha, p}$, $\hat{s}_{\alpha, p}$, or some other function.  Let $p^*$ be the index of the ``optimal'' norm.  What is optimal depends on $f$.  If $f$ is the p-value measure $s_p$ then the optimal norm minimizes the $p-value$.  However if instead $f$ is estimated power, then the optimal norm maximizes estimated power.     

% Define the test statistic $\hat{t}$ as 
% \begin{align*}
% \hat{t} = f_{p^*}(\boldsymbol{x}_1, \dots, \boldsymbol{x}_n)
% \end{align*}
% While it is not necessary to do this, the same $f$ used for norm selection will be used to define our test statistic.

\subsection{Obtaining the null distribution}
\label{ssec:obtaining_null}
The described procedure requires knowledge of the limiting distribution of $\sqrt{n}\hat{\boldsymbol{\psi}}$ when $P \in \mathscr{M}_0$.  To obtain an estimate of this limiting distribution, we require that each of the estimators $\hat{\boldsymbol{\psi}}_1, \dots, \hat{\boldsymbol{\psi}}_d$ of $\boldsymbol{\psi}_1, \dots, \boldsymbol{\psi}_d$ is asymptotically linear.  That is for each $j \in \{1, \dots, d\}$:
\begin{align*}
\hat{\psi}_j = \psi_j + \frac{1}{n}\sum_{i=1}^n D_j(\boldsymbol{x}_i) + o_p(1/\sqrt{n}) \text{ for some function } D_j
\end{align*}

Where the $D_j$ function will be referred to as an influence function. While this requirement may seem rigid, most standard measures of association are asymptotically linear, and there exists a rich and growing body of literature that describes asymptotically linear estimators and their corresponding functions \textcolor{blue}{Need citations here}.

% $\sqrt{n}\hat{\boldsymbol{\psi}}$, converges to a normal distribution with an estimable variance covariance matrix when properly centered and normalized.

When there is a fixed number of covariates, the Cramer-Wold device can be used to show that the vector of parameter estimates is asymptotically normal with mean zero, and variance covariance matrix given by $\Sigma = E_{\disto_0}\left[D(\rvo) D(\rvo)^\top \right]$:
% \textcolor{blue}{Note from Alex: do you mean $D(\rvo) D(\rvo)^\top$? This is what you would want if $D(\rvo)$ were a column vector, which would be more standard (similar comment in other places where `$\top$' is used)}

\begin{align*}
    \sqrt{n}\left(\hat{\boldsymbol{\psi}} - \boldsymbol{\psi}\right) \xrightarrow{d} \rvv \sim N\left(0, \Sigma\right)
\end{align*}

Under $H_0$, $\sqrt{n}\hat{\boldsymbol{\psi}}$ converges to $Z$, and $\Sigma$ can be approximated with $\widehat{\Sigma} = \frac{1}{n}\sum_{i = 1}^n D(\boldsymbol{x}_i) D(\boldsymbol{x}_i)^\top$.  In practice, a consistent estimator of $\vmat$, $\hat{\vmat}$ will be used in place of $\vmat$ for the calculation of $\tst$. Thus, the test statistic will be $\Gammafe(\tst)$ will be compared to $\Gammafe(\rvt)$.

\subsection{Using a permutation test for the test statistic}
While the above approach works asymptotically, there can be issues for small sample sizes.  To avoid inflated type one error, a permutation based test can be used.  Here, $\hat{\distv}^\#$ is used to define $\Gammaf$, and the test will compare $\Gamma_{\hat{\distv}}(\sqrt{n} \hat{\boldsymbol{\psi}})$ to $\Gamma_{\hat{\distv}}(\rvv^\#)$. To determine $\hat{\distv}^\#$, the $Y$'s from the observed data are permuted before calculating $\widehat \Sigma$.  Draws from $\rvv^\#$ are taken by permuting all of the $Y$'s of the observed data.  \textcolor{blue}{There are a few more complications here that need to be figured out.}

\section{Simulation Study}
\label{sec:sim_stdy}

\subsection{Correlation}

% In the first set of experiments, we compare our method to both a Bonferroni based method and a tailor made method.  The Bonferroni based method computes a p-value for each covariate using a marginal linear regression.  If any p-value is smaller than the Bonferroni adjusted cutoff value, the null hypothesis is rejected. The method outlined by \citep{zhang_comment_2015} is similar to our method, but uses a parametric estimate of the correlation instead of using an influence function based estimator.  Four versions of our test are displayed in these plots. Our test that uses the $\ell_2$ norm, our test that uses the $\ell_\infty$ or max norm, and our test that selects over five possible $\ell_p$ norms.  The last version of our test selecs over many different versions of the following norm : $\|x\|_k = \sum_{i = 1}^k x_{(i)}^2$ where $x^2_{(i)}$ are the ordered statistics of $x^2$.

% In each setting, each test is run 1000 times.  The proportion of tests that reject the null is plotted for each simulation setting and for each test.  

\begin{figure}[]
	\centering
\includegraphics[width = \linewidth]{uncor.jpg}
	\caption{Display of simulations for vector of covariates in this setting will be generated from a normal distribution with mean zero and a variance covariance of $\Sigma$ with $\Sigma_{ij}$ equal to $0$ when $i \neq j$ and equal to 1 when $i = j$. Three different models for the outcome of interest ($Y$) will are considered. Letting $\varepsilon \sim N(0, 1)$ and be independent of $X$, in the first model $Y = \varepsilon$, in the second $Y = X_1 / 4$, and in the third $Y = \sum_{k = 1}^{10} \beta_k X_k + \varepsilon$ where $\beta_k = 0.15$ for $k = \{1, \dots, 5\}$, and $\beta_k = -0.1$ for $k = \{6 \dots 10\}$. Sample sizes of $100$ and $200$, dimensions of $10$, $50$, $100$, $150$, and $200$ are considered.}
	\label{fig:uncor}
\end{figure}

\begin{figure}[]
	\centering
\includegraphics[width = \linewidth]{some_core.jpg}
	\caption{The same simulation settings as those used in Figure \ref{fig:uncor}, but $\Sigma_{ij} = 0.5$ for $i \neq j$.}
	\label{fig:somecor}
\end{figure}	

\begin{figure}[]
	\centering
\includegraphics[width = \linewidth]{lots_cor.jpg}
	\caption{The same simulation settings as those used in Figure \ref{fig:uncor}, but $\Sigma_{ij} = 0.8$ for $i \neq j$.}
	\label{fig:lotscor}
\end{figure}

% We find that our procedure uses obtains relatively good power across a variety of settings.  While in certain settings our test is beaten by that of Zhang and Laber, this is to be expected because their procedure is tailor made for the setting in which there is normal data that follows a linear model. We also find that the adaptive version o the test consitently outperforms the Bonferroni based method in all settings except those in which there is no between-covariate correlation.

In the above figure, the rejection rates of six different tests are shown for a wide variety of settings.  Data are generated from three different potential models.  In the first model no covariates are directly associated with the outcome ($H_0$ holds), in the second model a single covariate is strongly directly associated with the outcome ($H_0$ does not hold), and in the third model ten covariates are directly associated with the outcome ($H_0$ does not hold).  In all models the covariates are generated from a mean zero multivariate normal distribution $\Sigma$, where $\Sigma_{ij} = 0.8$ for $i \neq j$ and 1 for $i = j$.  Each color of dots represents a different type of test.  The red dots indicate a bonferroni adjusted marginal test for each covariates estimated correlation with the outcome. The light blue dots indicate performance of the test proposed by [Zhang and Laber]. The other colors correspond to different variants of our test. Dark green and yellow dots indicate the performance of our test using only the $\ell_2$ or maximum absolute value norm respectively.  The light green dots indicate the performance of our test when it adaptively select one of the $\ell_p$ norms. Brown dots indicate our tests performance when the test adaptively selects over various sum of squares norms. The dotted red line in each plot indicates the $0.05$ rejection rate that should be observed when $H_0$ holds.  

Because $H_0$ is true for model one, we expect the rejection rates for this model to be $0.05$.  Figures \ref{fig:uncor}, \ref{fig:somecor}, and \ref{fig:lotscor} show these rates are achieved by every testing procedure except the bonferroni based test which is somewhat conservative.  For the other two models, $H_0$ does not hold so these plots compare the powers between the different testing procedures.  The bonferroni based test has the lowest power in all of the considered settings and for larger numbers of covariates this differences is larger. All other tests have similar power in most settings, with the test proposed by [Zhang and Laber] performing slightly better in most settings where a difference exists.  All tests perform better for larger sample sizes, but tend to perform similarly for varying dimension and generating model.

One setting in which the adaptive test performs poorly is in settings in which a single covariate is correlated with the outcome of interest and no covariates are correlated as seen for model 2 in figure \ref{fig:uncor}. This behavior could be due to the parameter estimates inability to be sparse even in settings when sparsity occurs.  The many small errors in the parameter estimate across many covariates leads to a preference for the $\ell_2$ norm that obtains an overly optimistic estimate of power due to the accumulation of many small effects across all the covariates.

% A wide variety of simulation settings are considered in this section to show the breath of settings in which the adaptive test can be used.  The first setting considers independent draws from a random vector with mean zero and variance-covariance matrix $\Sigma$.  The measure of association between $Y$ and each covariate will be $\text{Cov}(Y, W_{j})/\text{Var}(W_j)$.  The dimension of the vector, denoted $d$ takes values $10, 50$, and $200$.  The number of correlated covariates will be zero, one, or $0.1, 0.2$, or $0.6$ of all the correlated covariates.  The correlation between each of the covariates will be either $0$ or $0.5$.  The sample sizes considered are $100$ and $500$.   

\subsection*{Two Phase Sampling Risk Ratio}
Here, we would expect that the $\ell_2$ norm would perform poorly in the model 2 setting while performing well in the model 3 setting, and that the max norm would perform poorly in the model three setting while performing well in the model two setting.  However, we see the opposite behavior.  Additionally, we find that our adaptive test performs well consistently across all alternatives. 

\textcolor{blue}{I will eventually just chose one of these figures because performance will be very similar between the two measures.  I just want to see how similar they are once I get a complete set of simulation results.}

\begin{figure}[H]
	\centering
\includegraphics[width = 0.7 \linewidth]{magnitude.pdf}
	\caption{Rejection rates across three different data generating mechanisms.  Here we are using the the multiplicative distance our test statistic is from obtaining 80\% power as our measure of performance.}
	\label{fig:magnitude}
\end{figure}

\begin{figure}[H]
	\centering
\includegraphics[width = 0.7 \linewidth]{power.pdf}
	\caption{Rejection rates across three different data generating mechanisms. Here we are using estimated power as the measure of performance.}
	\label{fig:magnitude}
\end{figure}

\section{Data Application}
\label{sec:data_app} 

\sh{\begin{enumerate}
	\item Data from Peter Gilbert
\end{enumerate}
}

\subsection*{Marginal Structural Model}

\section{Discussion}
\label{sec:discuss}


In this paper, we have discussed the difficulties of creating multivariate test of no correlated coefficients.  General methods are have wide applicability, but are often underpowered.  More powerful methods are frequently difficult to understand, and will only work properly in narrow settings for a single parameter of interest.  

We described a test that addresses these issues, by achieving comparable performance to taylor made methods, while being applicable in a wide variety of settings.  We have demonstrated the methods ability to compete with existing methods in section \ref{sec:sim_stdy}, and shown novel settings in which the method can also be performed.  Data example 2 was an example of testing a relatively simple parameter (a risk ratio) in a setting with a non-standard sampling scheme.  Data example 3 was an example of testing a more complex parameter (a parameter inside of a marginal structural model) for a straightforward sampling scheme.  

While our procedure does rely on asymptotic results to function properly for many parameters, in certain cases, a our test can be carried out using permutations of the data to obtain better finite sample properties.  This is done by repeating our estimation procedure many times after randomly permuting the outcome variable.  The method is used in our first data example, and could be used for our third data example.  However, in certain settings with more complicated sampling schemes, the permutation based test is more difficult to implement because many of the outcomes are not even observed.

While we have outlined some of the ways in which our method can be used, there are many other ways in which it can be extended improved. 

While we focused on the studying the limiting distribution of $\sqrt{n}\hat{\psi}$ once can also consider the limiting distribution of $\sqrt{n}\hat{\Sigma}^{-1/2}\hat{\psi}$.  This estimator will always have a multivariate normal limiting distribution with an identity covariance matrix, which simplifies notation by always having a single limiting distribution.  It also simplifies proofs, and potentially could allow for analytical solutions for which norm (or weighted average of norms) would provide the best power.  However, this estimator runs into issues when $p > n$ and $\hat{\Sigma}^{-1/2}$ becomes impossible to estimate because of the rank deficiency of the variance estimator.  While this problem still technically exist when indexing our $\Gamma$'s by $\hat{\Sigma}$, using a multiplier bootstrap to take draws from $N(0, \Sigma)$ avoids the computation issues of having a degenerate estimate of the limiting distributions variance matrix.

While changing the test statistic as described above has major hurdles to overcome to be implemented in $p > n$ settings, there are other extensions or generalizations of our procedure that are possible without much change.  While we considered one set of $\Gammaf$ funcitons in this article, any set of reasonable $\Gammaf$ functions could be used.  These $\Gammaf$ functions could even be picked by machine learning methods used for classification.  The classifier would provide a probability that any observation was generated from the null distribution.  Doing this provides a more rich set of $\Gammaf$ over which to select, and has the potential to provide large increases in test performance.  One would expect that a classifier would perform better better when data are generated at an alternative versus at the null.  Thus one could reject when the classification performance is especially large.  

While our procedure currently only selects a single norm to calculate the test statistic, it is possible to also consider an estimator that is a weighted average of the many different norms.  The weights would be estimates of the probability that each norm was optimal. These probabilities could be estimated by taking many draws from a normal with mean $\sqrt{n}\hat{\boldsymbol{\psi}}$ and variance $\hat{\Sigma}$ and finding the optimal norm for each of these draws.  This method while being more computationally costly, could potentially be shown to be optimal in the sense that it would maximize the average power of all tests based on the specified family of norms for each local alternative. 

While $\ell_p$ norms were used throughout this paper, one issue observed with our test is that it selects the $\ell_2$ norm more frequently than it should, likely because of the many small estimates for parameters that are not associated with the outcome.  One could consider a slightly modified norm that sets to zero all component values of the vector that are less than some value (say $2 \times \text{var}(X)\sqrt{n}$).  This would hopefully solve issues of small values close to zero causing incorrect selection of norms that perform well when there are many small effects.

\textcolor{blue}{add notes about limitation of our method (need to fin influence function)}

\textcolor{blue}{Multiple measures of association for each covariate.)}

\section{Conclusion}

\section{Appendix}

\subsection{Test Consistency}
\label{sec:test_cnsty}

\begin{theorem}
\label{thm:cnst}
Assume the performance metric of choice is $\hat{r}_{\alpha, p}(a)$, and each the norms $g_a$ considered have the following properties: 
\begin{align}
& \text{for } x \in \mathbb{R}^d, \text{ and } s, l > 0, s \cdot \max({\boldsymbol{x}}) \leq g_a(\boldsymbol{x}) \leq l \cdot d \cdot \max({\boldsymbol{x}}) \label{eqn:nrm_bounds}\\
& \text{for } s \leq 1, l \geq 1, g_a(s \cdot \boldsymbol{x}) \leq g_a(\boldsymbol{x}) \leq g_a(l \cdot \boldsymbol{x}) \label{eqn:linegrowth}
\end{align}
Then for all $P \not \in \mathscr{M}_0$ 
\begin{align*}
	Pr \left(\frac{1}{B}\sum_{k = 1}^B I\left\{\hat{T} \leq \hat{T}_k^{\#}\right\} < 0.05\right) \xrightarrow{p} 1 \text{ as } n \to \infty
\end{align*}
or Equivalently 

\begin{align*}
Pr\left(\hat{T} \leq F^{-1}_{\hat{T}^\#}(0.05)\right) \xrightarrow{p} 1 \text{ as } n \to \infty
\end{align*}
\end{theorem}

\begin{proof}
This proof will consist of three parts.  We will first show that as $n \to \infty, \hat{T}_a^\#$ becomes bounded away from $0$ for any valid norm.  Next we will show $\hat{T}_a$ converges to $0$ in probability for any valid norm.  Last we will show the two previous findings imply theorem \ref{thm:cnst}.

Let $P^\#_{\rvo}$ denote the distribution of the randomly permuted observations.  \sh{Because $Pr(Y^{\#}_i \indep \boldsymbol{W}^\#_{i}) \to 0$, $\boldsymbol{\Psi}(P^\#_{\rvo}) = \boldsymbol{0}$.  Additionally, $\sqrt{n}\left(\hat{\boldsymbol{\psi}}^\# -\boldsymbol{\psi}^\# \right) \xrightarrow{d} Z^\# \sim N\left(\boldsymbol{0}, \Sigma_{\text{perm}}\right)$}.
Define $\hat{T}_a^\# = \min_s\{s : g_a(s \cdot \sqrt{n}\hat{\boldsymbol{\psi}}^\#) \geq C_{0.95, a}^\# \}$ and $C^\#_{0.95, a}$ is $F^{-1}_{Z^\#}(0.95)$.  Since we know that $\psi^\# = \boldsymbol{0}$, it follows that $\sqrt{n}\hat{\boldsymbol{\psi}}^\# \overset{d}{\approx} Z^\#$.

Now, consider:
\begin{align*}
	\pr\left(\hat{T}^\#_a  > \epsilon\right) &= \pr\left(g_a\left(\epsilon \cdot \sqrt{n}\hat{\boldsymbol{\psi}}^\#\right) \leq C_{0.95, a}\right)\\
	& \geq \pr\left(\epsilon \cdot d \cdot  \max\left(\sqrt{n} \hat{\boldsymbol{\psi}}^\# \right)\leq C_{0.95, a}\right)\\
	& = \pr\left( \max\left(\sqrt{n} \hat{\boldsymbol{\psi}}^\# \right)\leq C_{0.95, a}/(\epsilon \cdot d)\right)
\end{align*}
Because $\max\left(\left|\sqrt{n} \hat{\boldsymbol{\psi}}^\#\right|\right)$ converges to a well defined, positive distribution as a result of the continuous mapping theorem, for each constant $c < 1$, we know there exists an $\epsilon_c$ such that $\pr\left(\hat{T}^\#_a  > \epsilon_c\right) \geq c$.  

%From the definition of $C_{0.95, a}^\#$ it follows that $Pr(g_a(\sqrt{n}\hat{\boldsymbol{\psi}}^\#) \leq C^\#_{0.95, a}) \to 0.95$.  For realizations in which $g_a(\sqrt{n}\hat{\boldsymbol{\psi}}^\#) \leq C^\#_{0.95, a}$ it must also be true that $\hat{t}^\# > 1$  from \eqref{eqn:linegrowth}. From the above logic it follows that $Pr(\hat{T}_a^\# > 1) \geq 0.95$ and thus $F^{-1}_{\hat{T}_a^\#}(0.05) \geq 1$ with a probability approaching 1.

Now, shifting our focus to $\hat{T}_a$, under alternatives, $\boldsymbol{\psi} \neq \boldsymbol{0}$. Define $\psi_{\max} = \max(\psi_1, \dots, \psi_d)$.  Using this knowledge, and \eqref{eqn:nrm_bounds}, note that 
\begin{align}
\pr\left(\hat{T}_a < \epsilon\right) &= \pr\left(g_a\left(\epsilon \cdot \sqrt{n} \hat{\boldsymbol{\psi}}\right) \geq C_{0.95, a}\right) \nonumber\\
&\geq \pr\left(\epsilon \cdot \sqrt{n}\max(\hat{\psi}_1, \dots, \hat{\psi}_d) \geq C_{0.95, a}\right) \nonumber \\
&= \pr\left(\max(\hat{\psi}_1, \dots, \hat{\psi}_d) \geq C_{0.95, a}/\left(\epsilon \cdot \sqrt{n}\right)\right) \nonumber \\
&\geq \pr\left(\max(\hat{\psi}_1, \dots, \hat{\psi}_d) \geq \psi_{\max}/2\right) \pr\left(\psi_{\max}/2\geq C_{0.95, a}/\left(\epsilon \cdot \sqrt{n}\right)\right) \label{eqn:prod_tlte} 
\end{align}
The first factor of the product in \eqref{eqn:prod_tlte} will converge to 1 as $n \to \infty$ from the consistency of $\hat{\boldsymbol{\psi}}$.  The second quantity will be equal to 1 for sufficiently large $n$.   Thus $\hat{T}_a \xrightarrow{p} 0$ under any alternative.   

It was shown that for each $a$ that $\hat{T}_a \xrightarrow{p} 0$.  This means that our adaptive estimator $\hat{T} \xrightarrow{p} 0$ as well.  Now, let $c = 0.05/k$ and $\epsilon_c$ be small enough that $\pr\left(\hat{T}^\#_a  > \epsilon_c\right) \geq 1 - (0.05/k)$.  The permutation version of the adaptive estimator $\hat{T}^\#$ has the property that
\begin{align*}
	\pr\left(\hat{T}^\# < \epsilon_c\right) \leq \pr(\hat{T}^\#_1 < \epsilon_c) + \dots + \pr(\hat{T}^\#_k < \epsilon_c) \leq 0.05,
\end{align*}
and the theorem's conclusion follows.
\end{proof}

\sh{\subsection{Unbiasedness at local alternatives}
Consider a local alternative in which the true value of $\psi$ is shrinking towards zero at a root $n$ rate: $\psi = \underline{h}/\sqrt{n}$.  We assume that each potential norm is convex.  This assumption can be relaxed to what was described by Eaton and Perlman (1991) (Concentration inequalitites for multivariate distributions: mv normal).  Under this local alternative, we will have $\sqrt{n} \hat{\psi} \xrightarrow{d} N(\underline{h}, \Sigma)$.    
Show that the test will reject the null with a probability greater than $\alpha$ for an $\alpha$ level test}

\begin{theorem}

Under local alternatives described above, 
\begin{align*}
	Pr_{\disto}\left(\Gamma^*_{\hat{\vmat}}(\sqrt{n}\hat{\boldsymbol{\psi}}) \leq F^{-1}_{\Gamma^*_{\hat{\vmat}}(\hat{\rvv})}(\alpha)\right) > \alpha,
\end{align*}
 
Where $\hat{\rvv} \sim \hat{\distv}$ and $\rvv \sim \distv$. Here (unlike other parts of the paper) small values of $\Gamma$ provide evidence against the null.  Values of $\Gamma$ can be thought of as similar to $p$-values.
\end{theorem}

\begin{lemma}
	\label{lemma:cont_of_gam}
	The function:
	\begin{align*}
		\Gamma_{\Sigma}(t) = Pr_\distv(\|\tilde\rvv + t\| > c_{0.8})  \text{ where }  c_{\vmat, 0.8} \equiv \min_{c}\{c : Pr_\distv(\|\tilde\rvv\| < c) \geq 0.8 \} \text{ and } \tilde\rvv \sim N(0, \Sigma)
	\end{align*}
	is continuous with respect to $\Sigma$ and $t$.  
\end{lemma}

This lemma will follow because $\Gamma$ is the integral of a composition of bounded, continuous functions.   

\begin{proof}
	The multivariate normal probability density function
\begin{align*}
	\phi(x, \mu, \Sigma) = \left(2\pi\right)^{-k} \text{det}\left({\Sigma}\right)^{- \frac{1}{2}} \exp\left(-\frac{1}{2} (x - \mu)^\top \Sigma^{-1}(x - \mu)i\right) 
\end{align*}	
is continuous with respect to both $\Sigma$ and $t$.

Since the exponential function, determinate, matrix inverses, and linear operators are all differentiable, they are also all continuous.  Because the multivariate normal pdf is the composition of continuous functions, it is also continuous.

It follows that if $\mu_n \rightarrow \mu$, and $\Sigma_n \rightarrow \Sigma$, then for each $x$, $\phi(x, \mu_n, \Sigma_n) \rightarrow \phi(x, \mu, \Sigma)$.
This result and the dominated convergence theorem imply the corresponding CDF's are also continuous \textcolor{blue}{($\phi(x, \hat{\mu}, \hat{\Sigma}) + \phi(x, \mu, \Sigma)$ can be used as the dominating measure): (dominating function shouldn't depend on $n$)}  To find a dominating measure, assume that $\vmat_n^{-1}$ is close enough to $\vmat^{-1}$ so the smallest eigenvalue is within $\varepsilon$ then bound the pdf using this fact and that $(x - \mu)^\top \Sigma^{-1}(x - \mu) = (x - \mu)^\top ADA^{-1}(x - \mu) \leq \|(x - \mu)\|^2 c$ where $c$ is the largest eigen value (can be done to provide oposite direction as well).

\begin{align*}
	\pf_{\hat{\Sigma}, \hat{\mu}}\left(t \right) = \int_{\|x\| < t} \phi(x, \hat{\mu}, \hat{\Sigma})dx \rightarrow \int_{\|x\| < t} \phi(x, \mu, \Sigma)dx = \Phi_{\Sigma, \mu}(t).
\end{align*}

\textcolor{blue}{Show (or find reference showing that) $\Phi_{\Sigma_n,0}^{-1}(0.8)\rightarrow \Phi_{\Sigma,0}^{-1}(0.8)$ when $\Sigma_n\rightarrow \Sigma$. E.g., implicit function theorem will do this}

Because the normal cdf is continuous, we know the normal quantile function will be continuous as well. Potentially we can prove the existence of the inverse using the fact the CDF has a bounded derivative on all of $\mathbb{R}$.  Let $\Sigma_n \rightarrow \Sigma$ and $h_n \rightarrow h$.  Also, let $Q_n$ be a normal distribution with mean zero and variance-covaraince $\Sigma_n$.  These findings imply the following: 

\begin{align*}
	\left|\Gamma_{\Sigma_n}(h) - \Gamma_{\Sigma}(h)\right| &= \left|Pr_{\distv_n}(\|\tilde\rvv + h\| > \Phi^{-1}_{\Sigma_n, 0}(0.8)) - Pr_{\distv}(\|\tilde\rvv + h\| > \Phi^{-1}_{\Sigma, 0}(0.8))\right|\\
	&= \left|\int_{\mathbb{R}^d} \phi(t, 0, \Sigma_n) I\{\|t+h\| > \Phi^{-1}_{\Sigma_n, 0}(0.8) \} - \phi(t, 0, \Sigma) I\{\|t+h\| > \Phi^{-1}_{\Sigma, 0}(0.8) \}dt \right| \\ 
	&= \left|\int_{\mathbb{R}^d \setminus \left\{t : ||t|| =  \Phi^{-1}_{\Sigma, 0}(0.8) \right\}} \phi(t, h, \Sigma_n) I\{\|t\| > \Phi^{-1}_{\Sigma_n, 0}(0.8) \} - \phi(t, h, \Sigma) I\{\|t\| > \Phi^{-1}_{\Sigma, 0}(0.8) \}dt \right| \\ 
	&\leq \int_{\mathbb{R}^d \setminus \left\{t : ||t|| =  \Phi^{-1}_{\Sigma, 0}(0.8) \right\}} |\phi(t, h, \Sigma_n) I\{\|t\| > \Phi^{-1}_{\Sigma_n, 0}(0.8) \} - \phi(t, h, \Sigma) I\{\|t\| > \Phi^{-1}_{\Sigma, 0}(0.8) \}|dt \\ 
\end{align*}
The above quantity converges to zero by the dominated convergence theorem.

\begin{align*}
	& = \int_{\|t\| > \Phi^{-1}_{\Sigma_n, 0}(0.8), \Phi^{-1}_{\Sigma, 0}(0.8)} |\phi(t, h, \Sigma_n) - \phi(t, h, \Sigma)|dt + \\
	& \hspace{5em} \int_{\Phi^{-1}_{\Sigma, 0}(0.8) \geq \|t\| > \Phi^{-1}_{\Sigma_n, 0}(0.8)} |\phi(t, h, \Sigma_n)|dt + \int_{\Phi^{-1}_{\Sigma_n, 0}(0.8) \geq \|t\| > \Phi^{-1}_{\Sigma, 0}(0.8)} | \phi(t, h, \Sigma)|dt \\
	& \leq \int_{\|t\| > \Phi^{-1}_{\Sigma, 0}(0.8)} |\phi(t, h, \Sigma_n) - \phi(t, h, \Sigma)|dt + \\
	& \hspace{5em} \int_{\Phi^{-1}_{\Sigma, 0}(0.8) \geq \|t\| > \Phi^{-1}_{\Sigma_n, 0}(0.8)} \phi(t, h, \Sigma_n)dt + \int_{\Phi^{-1}_{\Sigma_n, 0}(0.8) \geq \|t\| > \Phi^{-1}_{\Sigma, 0}(0.8)} \phi(t, h, \Sigma)dt \\
	& = \int_{\|t\| > \Phi^{-1}_{\Sigma, 0}(0.8)} |\phi(t, h, \Sigma_n) - \phi(t, h, \Sigma)|dt + \\
	& \hspace{5em} \Phi_{\Sigma_n, h}(\Phi^{-1}_{\Sigma, 0}(0.8)) - \Phi_{\Sigma_n, h}(\Phi^{-1}_{\Sigma_n, 0}(0.8)) + \Phi_{\Sigma, h}(\Phi^{-1}_{\Sigma, 0}(0.8)) - \Phi_{\Sigma, h}(\Phi^{-1}_{\Sigma_n, 0}(0.8)) \\
	& = \int_{\|t\| > \Phi^{-1}_{\Sigma, 0}(0.8)} |\phi(t, h, \Sigma_n) - \phi(t, h, \Sigma)|dt + \\
	& \hspace{5em} \Phi_{\Sigma_n, h}(\Phi^{-1}_{\Sigma, 0}(0.8)) - \Phi_{\Sigma_n, h}(\Phi^{-1}_{\Sigma_n, 0}(0.8)) - \left(\Phi_{\Sigma, h}(\Phi^{-1}_{\Sigma, 0}(0.8)) - \Phi_{\Sigma, h}(\Phi^{-1}_{\Sigma_n, 0}(0.8))\right)+\\
	& \hspace{5em} \left(\Phi_{\Sigma, h}(\Phi^{-1}_{\Sigma, 0}(0.8)) - \Phi_{\Sigma, h}(\Phi^{-1}_{\Sigma_n, 0}(0.8))\right) +  \Phi_{\Sigma, h}(\Phi^{-1}_{\Sigma, 0}(0.8)) - \Phi_{\Sigma, h}(\Phi^{-1}_{\Sigma_n, 0}(0.8)) \\
	& = \int_{\|t\| > \Phi^{-1}_{\Sigma, 0}(0.8)} |\phi(t, h, \Sigma_n) - \phi(t, h, \Sigma)|dt + \\
	& \hspace{5em} \left(\Phi_{\Sigma_n, h}(\Phi^{-1}_{\Sigma, 0}(0.8)) - \Phi_{\Sigma, h}(\Phi^{-1}_{\Sigma, 0}(0.8)) \right) - \left(\Phi_{\Sigma_n, h}(\Phi^{-1}_{\Sigma_n, 0}(0.8)) -  \Phi_{\Sigma, h}(\Phi^{-1}_{\Sigma_n, 0}(0.8))\right)+\\
% Also, because $\Phi_{\Sigma, \mu}(t)$ has a bounded derivative, it is uniformly continuous.
	& \hspace{5em} \left(\Phi_{\Sigma, h}(\Phi^{-1}_{\Sigma, 0}(0.8)) - \Phi_{\Sigma, h}(\Phi^{-1}_{\Sigma_n, 0}(0.8))\right) +  \Phi_{\Sigma, h}(\Phi^{-1}_{\Sigma, 0}(0.8)) - \Phi_{\Sigma, h}(\Phi^{-1}_{\Sigma_n, 0}(0.8)) \\
\end{align*}
% \textcolor{blue}{LHS should be $|\Gamma_{\Sigma_n}(x_n) - \Gamma_{\Sigma}(x)|$}

Taking the limit as $n \to \infty$ of the quantity above, we find that the first term is zero by dominated convergence theorem.  The other four terms are also zero because of the uniform continuity of $\Phi_{\Sigma, x}$ and continuity of $\Phi^{-1}_{\Sigma, x}$
\end{proof}

\begin{lemma}
Under local alternatives, 
	\begin{align*}
		\Gamma^*_{\hat{\Sigma}}(\sqrt{n} \hat{\boldsymbol{\psi}}) \xrightarrow{d} \Gamma^*_{\Sigma}(Z + \underline{h}) 
	\end{align*}
\end{lemma}

\begin{proof}
	It was shown in Lemma \ref{lemma:cont_of_gam} that for each norm $\|\cdot\|$, $\Gamma_{\Sigma}(x)$ is continuous with respect to $\Sigma$ and $x$.  This finding, the continuity of the max function, and because a composition of continuous functions is also continuous it follows that

	\begin{align*}
		\Gamma^*_{\Sigma} \equiv \max\left\{\Gamma_{1, \Sigma}, \dots, \Gamma_{d, \Sigma}\right\}
	\end{align*}
	is also continuos with respect to $\Sigma$ and $x$. Under local alternatives, $\sqrt{n} \hat{\boldsymbol{\psi}} \xrightarrow{d} Z + \underline{h}$ and $\hat{\Sigma} \xrightarrow{p} \Sigma$.   It follows from the continuous mapping theorem that $\Gamma^*_{\hat{\Sigma}}(\sqrt{n} \hat{\boldsymbol{\psi}}) \xrightarrow{d} \Gamma^*_{\Sigma}(Z + \underline{h}) $
\end{proof}

It has now been establish that under local alternatives the distribution of our estimate converges to $\Gamma^*_{\Sigma}(Z + \underline{h})$.  Denote the CDF of $\Gamma^*_{\Sigma}(Z + t)$ this distribution by $F_{\Sigma, t}$ and the corresponding quantile function of the distribution by $F^{-1}_{\Sigma, t}$.  

To prove unbiasedness at local alternatives, we show \textcolor{blue}{$>$?}
\begin{align*}
	F_{\Sigma, t}(F^{-1}_{\Sigma, 0}(1 - \alpha)) \geq F_{\Sigma, 0}(F^{-1}_{\Sigma, 0}(1 - \alpha)) = \alpha
\end{align*}
using results from the \citep{anderson_integral_1955} manuscript.
% \textcolor{blue}{What is $\rvv^* \overset{d}{=} \rvv$ telling us? Don't we already know that the probabilities over both of these quantities are indexed by $Q$? Also, why use different notation at all? Why not just $\rvv^*$ for both? Here because $c_{0.8}$ is not random so this is distinction does not need to be made.}
% In this scenario, the test can be carried out by comparing our test statistic, $\Gamma_\vmat(\sqrt{n}\hat{\boldsymbol{\psi}}, \|\cdot\|)$ to $\Gamma_\vmat(\rvv, \|\cdot\|).$


A result of \citep{anderson_integral_1955} is that for two centrally symmetric, unimodal functions, $f_1(x)$ and  $f_2(x)$, the convolution,  
\begin{align*}
	g(\theta) = \int f_1(x) f_2(x - \theta)
\end{align*}
is centrally symmetric and ray decreasing.  A function $f :\mathbb{R}^p\rightarrow\mathbb{R}$ is centrally symmetric if $f(-x) = f(x)$ for every $x$.  A function is unimodal if for every $k$, the set $\{x : f(x) \geq k\}$ is convex. A function $f$ on $\mathbb{R}^p$ is ray decreasing if for every $x$ in $\mathbb{R}^p$, the function $g(\beta) = f(\beta x), \beta \in \mathbb{R}$ is a decreasing function of $\beta$.

For now, we will assume that.  We assume that our tests will always be define our tests in such a way that we reject the null when $\Gamma^*_{\hat{\vmat}}(\sqrt{n} \hat{\boldsymbol{\psi}}) > c$ for some $c$. It is expected that $\Gamma^*_{\vmat}(x)$  increases as $x$ moves away from the origin. For the purposes of this proof it will be useful to define new functions 

\begin{align*}
	\gi_{i, \Sigma} \equiv \left(\gi_{i, \Sigma}\right)^{-1} \text{ and } \gi_{\Sigma}^* = \min\left\{\gi_{1, \Sigma}, \dots, \gi_{d, \Sigma}\right\}
\end{align*} 

that decrease as $x$ moves away from the origin.   

\begin{lemma}
	\label{lemma:min_cent_sym_unm}
	Let $\gi_1, \dots, \gi_d$ all be centrally symmetric, unimodal functions.  Then $\gi^* = \min\left\{\gi_1, \dots, \gi_d\right\}$ is also centrally symmetric and unimodal.
\end{lemma}
\begin{proof}
Because each $\gi_i$ is a centrally symmetric and $\gi^*$ is a function of $x$ only through the $\gi_i$'s, $\gi^*$ is also centrally symmetric:
	\begin{align*}
	\gi^*(x) = \min\left\{\gi_1(x), \dots, \gi_d(x)\right\} = \min\left\{\gi_1(-x), \dots, \gi_d(-x)\right\} = \gi^*(-x)
	\end{align*}
The set $M^* \equiv \{x : \gi^*(x) \geq k\}$ contains all of the $x$ for which $\gi_i(x) \geq k$ for all $i \in \{1, \dots, d\}$.  Thus $M^*$ is the intersection of all sets $M_i \equiv \{x : \gi_i(x) \geq k\}$.  Each $M_i$ is convex because each $\gi_i$ is unimodal, and the intersection of a countable number of convex sets is convex.  Thus $M^*$ is convex and $\gi^*$ is unimodal.
	% \begin{align*}
	% 	\left\{x : \gi^*(x) \geq k\right\} = \left\{x : \gi_1(x) \geq k, \dots, \gi_d(x) \geq k\right\} = \bigcap_{i = 1}^d \left\{x : \gi_i(x) \geq k \right\}
	% \end{align*}
\end{proof}

\begin{lemma}
	\label{lemma:indct_cent_sym_unm}
	Let $f(x)$ be a centrally symmetric, unimodal function.  Then $g(x) = I\left\{f(x) \geq c\right\}$ where $c \in \mathbb{R}$ is also centrally symmetric and unimodal. 
\end{lemma}
\begin{proof}
Because $f$ is centrally symmetric and $g$ is a function of $x$ only through $g$, $g$ is also centrally symmetric:
	\begin{align*}
	g(x) = I\left\{f(x) \geq c\right\} = I\left\{f(-x) \geq c\right\} = g(-x)
	\end{align*}

	If $k < 0$ or $k > 1$, then $\left\{x : g(x) \geq k\right\}$ will be the empty set or all of $\mathbb{R}^d$ respectively, and both sets are convex.  
	
	Otherwise, the set $\left\{x : g(x) \geq k\right\} = \left\{x : I\left\{f(x) \geq c\right\} \geq k\right\}$. The indicator function will be greater than or equal to $k$ whenever $f(x) \geq c$, so $\left\{x : g(x) \geq k\right\} =\left\{x : f(x) \geq c\right\}$
	which is convex because $f$ is unimodal.

	Thus $g$ is unimodal.
\end{proof}

Because each $\gi_{\Sigma, i}$ is unimodal and centrally symmetric, lemma \ref{lemma:min_cent_sym_unm} implies $\gi^*_{\Sigma, i}$ is also unimodal and centrally symmetric. It follows from lemma \ref{lemma:indct_cent_sym_unm} and the previous finding that $I\left\{\gi^*_Q(x) \geq c_{0.95}\right\}$ is centrally symmetric and unimodal.  

\begin{align*}
	\int I\left\{\Gamma^*_Q(x) > c_{0.95}\right\} \phi(x - \mu) dx &= \int I\left\{\gi^*_Q(x) < c_{0.95}^{-1}\right\} \phi(x - \mu) dx\\
	& = \int \left(1 - I\left\{\gi^*_Q(x) \geq c^{-1}_{0.95}\right\}\right)\phi(x - \mu) dx
	\\
	&= 1 - \int I\left\{\gi^*_Q(x) \geq c^{-1}_{0.95}\right\} \phi(x - \mu) dx
\end{align*}
Since the subtracted quantity is decreasing by \citep{anderson_integral_1955}, the quantity as a whole will be increasing.  Thus local power is obtained.

While each set of performance measures will require a proof that they are centrally symmetric and unimodal, we will show that the estimate power performance measure is centrally symmetric and unimodal. Consider the power function.

\begin{align*}
	\Gammaf(\mu) & = \int I\left\{\|x\|_p > c\right\} \phi_{\Sigma}(x - \mu)
\end{align*}

Consider two values $\mu_1$, $\mu_2$ such that $ \Gammaf(\mu_1), \Gammaf(\mu_2) \geq k$.  Now, consider $\Gammaf\left(t\mu_1 + (1 - t)\mu_2\right)$

\begin{align*}
	\Gammaf\left(t\mu_1 + (1 - t)\mu_2\right) & = \int I\left\{\|x\|_p > c\right\} \phi_{\Sigma}(x - t\mu_1 + (1 - t)\mu_2)dx \\
	& =  \int I\left\{\|x\|_p > c\right\} \phi_{\Sigma}(t(x - \mu_1) + (1 - t)(x - \mu_2))dx\\
	& \geq  \int I\left\{\|x\|_p > c\right\} \left[t\phi_{\Sigma}(x - \mu_1)) + (1 - t)\phi_{\Sigma}(x - \mu_2)\right]dx \\
	& = t \int I\left\{\|x\|_p > c\right\} \phi_{\Sigma}(x - \mu_1))dx + (1 - t) \int I\left\{\|x\|_p > c\right\} \phi_{\Sigma}(x - \mu_2)dx \\
	& = t \Gammaf(\mu_1) + (1 - t) \Gammaf(\mu_2) \geq k	
\end{align*}
The inequality on the third line comes from the fact that multivariate normal pdf is concave.

\sh{Proof Outline: \\
\begin{itemize}
	\item Under local alternatives, $\sqrt{n}\hat{\boldsymbol{\psi}} \xrightarrow{d} N(\boldsymbol{c}, \Sigma)$
	\item With a large enough sample size, and enough MC draws, we have that for each norm: 
    \item \textcolor{blue}{Think of Gamma as a funciton indexed by $\Sigma$ to allow it to be similar.  Also make some assumptions about how smooth $\gi$ is with respect to this parameters. For any given value of the input ($t$) think about }
	\item \textcolor{blue}{Right now we have given up on proving things for the permutation test, but we may try to do it again at some point.} The permutation test statistic will converge in distribution to a standard normal.  This paper should help: \citep{omelka_testing_2012}
	\item For each norm selected, \citep{gupta_inequalitites_1972} states that the power will be non-decreasing as long as the rejection region is convex (This should be true most of our rejection regions), and the probability density is decreasing away from the mean (which is true of a normal distribution).  
	\item Show that for $n$ large, the norm is selected to give the best power.  Thus since each norm obtains local power, the adaptive test will also obtain local power for $n$ large.
\end{itemize}} 
\begin{align*}
	\mathcal{L}\left(\Gamma_{\hat{\vmat}}\left(\sqrt{n} \hat{\boldsymbol{\Psi}}(\boldsymbol{X})\right), \Gamma_{\vmat}\left(\sqrt{n} \hat{\boldsymbol{\Psi}}(\boldsymbol{X})\right)\right) \rightarrow 0
\end{align*}

\sh{\subsection{Consistency of norm selection} 
This proof would likely be difficult and require some slower than $\sqrt{n}$ covergence rates of the local alternative considered.  Under fixed alternatives, all norms perform equally perfectly}

\sh{\subsection{Type 1 error control}
Proof is so short, I am not sure if it is worth including
Under the null, $Y \indep \rvo$.  Thus the test statistic will be taken from the same distribution as all of the permutation based test statistics used to estimate the distirbution of the test statistic under the null.  Therefore, as $B$ grows, $Pr(T_n \geq F_{T^\#_{k,n}|\rvo_n}(0.95)) \Rightarrow 0.05$
}

%However, determining which observations should be considered more extreme is not straightforward.  
%One way to solve this problem is by using a norm to transform all of your data to $\mathbb{R}$ instead of $\mathbb{R}^d$.  After taking the norm of your vector of parameter estimates and your estimated limiting distribution you can create a cutoff for your test.
% People are interested in discovering if their data about $\rvo$ could be predictive of some outcome $Y$.  While many there have been many procedures that have come before this one, the procedure outlined in this article has a few distinct advantages that are valuable to scientists.  

% For each covariate of interest, we can specify a measure of association between the covariate of interest and the outcome which is defined as a parameter of the data generating mechanism.  This measure of association can be estimated for each covariate of interest giving us a parameter for each covariate of interest. The null hypothesis which we will be testing is whether or not all of these parameters are equal to zero (or some other value which indicates no association exists).

% As with most test, our test will calculate a test statistic, and then estimate the limiting distribution of this test statistic under the null. Obtaining the test statistic is a three step procedure.  The first step is to obtain the limiting distribution of the vector of parameter estimates under the null.  This first step is made possible through the use of influence functions and a multiplier bootstrap.  This technique is applicable for a wide range of parameters that measure association.  Next, a norm is selected from a range of possible norms based on the norms' approximated performance under various alternatives.  After this, the selected norm is applied to the vector of parameter estimates and transformed by a function (which could be dependent on the norm) to give us our test statistic.  

% There are two options for obtaining the limiting distribution of this test statistic under the null.  The first is to use a parametric bootstrap.  We can take a single draw from the limiting distribution of our vector of parameter estimates under the null and treat this draw as if it were our vector of parameter estimates.  Using the same three step procedure described above we can obtain a single draw from the limiting distribution of our test statistic under the null.  This procedure can be repeated many times over to obtain the limiting distribution of our test statistic under the null.  The other option is to randomly permute our outcome variable and use this data to estimate the vector of parameter estimates.  Passing this vector through the three step procedure, we obtain a draw from the limiting distribution under the null.  Many draws can be obtained by taking many draws coming from different permutations of the outcome variable.  

% The permutation technique is less prone to inflated type 1 error rates, but comes at the cost of larger computational burden.

% The procedure is fast because estimation of the limiting distribution is done using a multiplier bootstrap.  The procedure's use of influence functions allows scientists to define the parameter of association that is best suited to answering their scientific question.  Adaptive selection of the norm used improves the testing procedure's ability to detect an association when one exists for a wide variety of data generating mechanisms.  Simulating the test-statistic generating procedure under the null using a permutation guarantees family-wise error control. 
\bibliography{paper_bib}

\end{document}


% \begin{lemma} \textcolor{red}{plus $h$?}
% 	\begin{align*}
% 	\Gamma^*_{\hat{\vmat}}(\sqrt{n}\hat{\boldsymbol{\psi}})	\xrightarrow{d}\Gamma^*_{\vmat}(\rvv + \underline{h}) \text{ and } F^{-1}_{\Gamma^*_{\hat{\vmat}}(\hat{\rvv})}(\alpha) \rightarrow  F^{-1}_{\Gamma^*_{\vmat}(\rvv) }(\alpha)
% 	\end{align*}
% \end{lemma}
% \textcolor{blue}{
% 	The proofs of these will be similar, but I am not sure quite how to do it.  While the following expansion is useful for me, I am not sure how to deal with each part:
% 	\begin{align*}
% 		Pr_{\disto}(\Gamma^*_{\hat{\vmat}}(\sqrt{n}\hat{\boldsymbol{\psi}}) > t) - Pr_{\disto}(\Gamma^*_{\vmat}(\vmat) > t) & = Pr_{\disto}(\Gamma^*_{\hat{\vmat}}(\sqrt{n}\hat{\boldsymbol{\psi}}) > t) - Pr_{\disto}(\Gamma^*_{\hat{\vmat}}(\vmat) > t) - \left( Pr_{\disto}(\Gamma^*_{\vmat}(\vmat) > t) - Pr_{\disto}(\Gamma^*_{\hat{\vmat}}(\vmat) > t) \right)	
% 	\end{align*}
% 	We can make arguments that the second two terms will cancel because of an assumed continuity of our function $\Gamma^*$ with respect to $Q$.  We can also think of $\Gamma^*$ as being indexed by $Q$'s covariance matrix instead of its entire distribution to make more simple continuity arguments.\\
% 	However, I am not exactly sure how to argue that the first two terms cancel.  Initially I want to use the continuous mapping theorem.  Because the function itself changes with $n$ and is random, I am not sure what to do.  If we again use the continuity of $\Gamma$ could we argue that the values of the random functions will be close to the values of the non-random funcitons (where $\hat\distv$ is replaced by $\distv$)\\
% 	My other idea was to assume that the function class of $\Gamma*$ is $P$-Donsker, but I think this may be overly complicated, and not necessary.
% }

% For fixed every $t$,  
% \begin{align*}
% \Gamma^*_{\hat{\vmat}}(t) \rightarrow \Gamma^*_{\vmat}(t)	\text{ if } \Gamma_{i, \hat{\vmat}}(t) \rightarrow \Gamma_{i, \vmat}(t) \text{ for every } i \in \{1, 2, \dots, d\}
% \end{align*}

% The function defined in \eqref{gamma:pow} satisfy this condition

% \begin{lemma}
% 	Let $t$ be fixed.  By the consistency of our variance estimator, we know that $\hat{Z} \xrightarrow{d} Z$, and by continuous mapping theorem $\|\hat{Z}\| \xrightarrow{d} \|Z\|$.  Thus it can be shown that $C_{\hat{\vmat}, 0.8} \rightarrow C_{\vmat, 0.8}$ because the distribution functions for both $\hat{\rvv}$ and $\rvv$ are continuous.  

% 	The above findings and Slutsky's theorem imply that $\|\hat{Z} + \underline{t}\| - C_{\hat{\vmat}, 0.8} \xrightarrow{d} \|Z - \underline{t}\| + C_{\vmat, 0.8}$ so: 
% \begin{align*}
% 	&\Gamma_{i, \hat{Q}}(t) = Pr_{\hat{\distv}}\|\left(\hat{Z} + \underline{t}\| > C_{\hat{\distv}, 0.8}\right) = Pr_{\hat{\distv}}\left(\|\hat{Z} + \underline{t}\| - C_{\hat{\distv}, 0.8} > 0\right) \rightarrow\\ 
% 	& Pr_{\distv}\left(\|Z + \underline{t}\| - C_{\distv, 0.8} > 0\right) = Pr_{\distv}\left(\|Z + \underline{t}\| > C_{\distv, 0.8}\right) =\Gamma_{i, Q}(t) 
% \end{align*}
% % \begin{align*}
% % 	Pr(\|\hat{\rvv}\| > c^-_{0.8}) \rightarrow Pr(\|\rvv\| > c^-_{0.8}) < 0.8.
% % \end{align*}
% \end{lemma}

% Note that above while our test was defined by rejecting or accepting based on if $\sqrt{n}\hat{\psi} \in \Theta_0$, we can also think about defining our test decision based on if some function, say $\Gamma_P(x, \|\cdot\|)$ that maps from a value and distribution in $\mathbb{R}^d$, and a norm $\|\cdot\|$ to a real value is greater than zero.

% Above we used a norm to constrain the considered rejection regions and allow for a test based on the value of $\sqrt{n}\hat{\boldsymbol{\psi}}$ under the norm the distribution of $\rvv$ under the norm, both of which reside in $\mathbb{R}$.
%the quantity could also be used for a test, and since we know $pr_q(\|\rvv\|_2 \geq \|\sqrt{n} \hat{\boldsymbol{\psi}}(\rvo)\|_2)$ will be uniformly distributed on $(0, 1)$ asymptotically under the null, we can reject $h_0$ whenever this value is less than $\alpha$.
% as an example, a test is defined by a norm $p$ and level $\alpha$ which define the cutoff value;
% \begin{align*}
% 	c_{p, \alpha} = \min\left\{c : \frac{1}{b}\sum_{b = 1}^b i\left\{\|\boldsymbol{z}_b\|_p \leq c\right\} \leq \alpha\right\},
% \end{align*}
% where $z_1, \dots, z_b$ are draws from $q$. reject the null if $\|\sqrt{n}\hat{\boldsymbol{\psi}}\|_p > c_{p, \alpha}$. 
%the quantity could also be used for a test, and since we know $pr_q(\|\rvv\|_2 \geq \|\sqrt{n} \hat{\boldsymbol{\psi}}(\rvo)\|_2)$ will be uniformly distributed on $(0, 1)$ asymptotically under the null, we can reject $h_0$ whenever this value is less than $\alpha$.
% as an example, a test is defined by a norm $p$ and level $\alpha$ which define the cutoff value;
% \begin{align*}
% 	c_{p, \alpha} = \min\left\{c : \frac{1}{b}\sum_{b = 1}^b i\left\{\|\boldsymbol{z}_b\|_p \leq c\right\} \leq \alpha\right\},
% \end{align*}
% where $z_1, \dots, z_b$ are draws from $q$. reject the null if $\|\sqrt{n}\hat{\boldsymbol{\psi}}\|_p > c_{p, \alpha}$. 

% In the univariate case, once a measure of association is selected, standard approaches exist to construct asymptotically valid (and sometimes optimal) tests \citep{neyman_jerzy_ix._1933}. In this setting, a test is considered optimal if it achieves the largest power for every alternative compared to every other test with the same type one error.  However, scientists are often interested if any of a multitude of covariate are associated with the outcome of interest.  Even in the case with two covariates, a test with the above optimal property no longer exists and the alternative plays a larger role in which tests are more or less powerful.

% To illustrate this, consider two tests with equal type one error rates. Both test the null that both of two association measures $\psi_1$, $\psi_2$ are zero. The first test focuses only on the $\psi_1$ and the second test considers both $\psi_1$ and $\psi_2$.  When the $\psi_1$ is non-zero $\psi_2$ is zero the first test can outperform all other tests.  Conversely when only $\psi_2$ is non-zero the first test will be outperformed by the second test.  Additionally in this setting, the power of these tests will be tied to the covariance between the covariates (a problem that did not exist for a single covariate).  These two factors make it difficult to define a test that performs well across all possible alternatives.  
% Even coming up with optimal properties for tests becomes difficult since it must make assumptions about the true data-generating mechanism. 