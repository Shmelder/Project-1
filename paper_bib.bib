
@article{donoho_higher_2004,
	title = {Higher criticism for detecting sparse heterogeneous mixtures},
	volume = {32},
	issn = {0090-5364, 2168-8966},
	url = {https://projecteuclid.org/euclid.aos/1085408492},
	doi = {10.1214/009053604000000265},
	abstract = {Higher criticism, or second-level significance testing, is a multiple-comparisons concept mentioned in passing by Tukey. It concerns a situation where there are many independent tests of significance and one is interested in rejecting the joint null hypothesis. Tukey suggested comparing the fraction of observed significances at a given α-level to the expected fraction under the joint null. In fact, he suggested standardizing the difference of the two quantities and forming a z-score; the resulting z-score tests the significance of the body of significance tests. We consider a generalization, where we maximize this z-score over a range of significance levels 0{\textless}α≤α0. We are able to show that the resulting higher criticism statistic is effective at resolving a very subtle testing problem: testing whether n normal means are all zero versus the alternative that a small fraction is nonzero. The subtlety of this “sparse normal means” testing problem can be seen from work of Ingster and Jin, who studied such problems in great detail. In their studies, they identified an interesting range of cases where the small fraction of nonzero means is so small that the alternative hypothesis exhibits little noticeable effect on the distribution of the p-values either for the bulk of the tests or for the few most highly significant tests. In this range, when the amplitude of nonzero means is calibrated with the fraction of nonzero means, the likelihood ratio test for a precisely specified alternative would still succeed in separating the two hypotheses. We show that the higher criticism is successful throughout the same region of amplitude sparsity where the likelihood ratio test would succeed. Since it does not require a specification of the alternative, this shows that higher criticism is in a sense optimally adaptive to unknown sparsity and size of the nonnull effects. While our theoretical work is largely asymptotic, we provide simulations in finite samples and suggest some possible applications. We also show that higher critcism works well over a range of non-Gaussian cases.},
	pages = {962--994},
	number = {3},
	journaltitle = {The Annals of Statistics},
	shortjournal = {Ann. Statist.},
	author = {Donoho, David and Jin, Jiashun},
	urldate = {2019-04-18},
	date = {2004-06},
	langid = {english},
	mrnumber = {MR2065195},
	zmnumber = {1092.62051},
	keywords = {combining many p-values, Multiple comparsions, normalized empirical process, sparse normal means, thresholding},
	file = {Snapshot:/Users/adamelder/Zotero/storage/2CT4DEAE/Donoho and Jin - 2004 - Higher criticism for detecting sparse heterogeneou.html:text/html}
}

@article{mckeague_adaptive_2015,
	title = {An Adaptive Resampling Test for Detecting the Presence of Significant Predictors},
	volume = {110},
	issn = {0162-1459},
	url = {https://amstat.tandfonline.com/doi/abs/10.1080/01621459.2015.1095099},
	doi = {10.1080/01621459.2015.1095099},
	abstract = {This article investigates marginal screening for detecting the presence of significant predictors in high-dimensional regression. Screening large numbers of predictors is a challenging problem due to the nonstandard limiting behavior of post-model-selected estimators. There is a common misconception that the oracle property for such estimators is a panacea, but the oracle property only holds away from the null hypothesis of interest in marginal screening. To address this difficulty, we propose an adaptive resampling test ({ART}). Our approach provides an alternative to the popular (yet conservative) Bonferroni method of controlling family-wise error rates. {ART} is adaptive in the sense that thresholding is used to decide whether the centered percentile bootstrap applies, and otherwise adapts to the nonstandard asymptotics in the tightest way possible. The performance of the approach is evaluated using a simulation study and applied to gene expression data and {HIV} drug resistance data.},
	pages = {1422--1433},
	number = {512},
	journaltitle = {Journal of the American Statistical Association},
	shortjournal = {Journal of the American Statistical Association},
	author = {{McKeague}, Ian W. and Qian, Min},
	urldate = {2019-04-18},
	date = {2015-10-02},
	keywords = {Mult\_hyp},
	file = {Accepted Version:/Users/adamelder/Zotero/storage/XZ3IXPR6/McKeague and Qian - 2015 - An Adaptive Resampling Test for Detecting the Pres.pdf:application/pdf;Snapshot:/Users/adamelder/Zotero/storage/D53Q4FKN/01621459.2015.html:text/html}
}

@article{s._holland_improved_1988,
	title = {Improved Bonferroni-Type Multiple Testing Procedures},
	volume = {104},
	doi = {10.1037/0033-2909.104.1.145},
	abstract = {[Correction Notice: An erratum for this article was reported in Vol 104(2) of Psychological Bulletin (see record
2008-06007-001). An error was made in the author note on page 145. Correspondence should be addressed to Burt S. Holland, Department of Statistics, Temple University, Speakman Hall (006-00), Philadelphia, Pennsylvania 19122. Margaret {DiPonzio} Copenhaver is now at Merck Sharp \& Dohme Research Laboratories, West Point, Pennsylvania.] The Bonferroni multiple comparisons procedure is customarily used when doing several simultaneous tests of significance in relatively nonstandard situations in which other methods do not apply. We review some new and improved competitors to the Bonferroni procedure, that although constraining generalized Type I error probability to be at most α, afford increased power in exchange for increased complexity in implementation. An improvement to the weighted form of the Bonferroni procedure is also presented. Several data sets are reanalyzed with the new methods. ({PsycINFO} Database Record (c) 2012 {APA}, all rights reserved)},
	pages = {145--149},
	journaltitle = {Psychological Bulletin},
	shortjournal = {Psychological Bulletin},
	author = {S. Holland, Burt and {DiPonzio} Copenhaver, Margaret},
	date = {1988-07-01}
}

@article{szekely_brownian_2009,
	title = {Brownian distance covariance},
	volume = {3},
	issn = {1932-6157, 1941-7330},
	url = {https://projecteuclid.org/euclid.aoas/1267453933},
	doi = {10.1214/09-AOAS312},
	abstract = {Distance correlation is a new class of multivariate dependence coefficients applicable to random vectors of arbitrary and not necessarily equal dimension. Distance covariance and distance correlation are analogous to product-moment covariance and correlation, but generalize and extend these classical bivariate measures of dependence. Distance correlation characterizes independence: it is zero if and only if the random vectors are independent. The notion of covariance with respect to a stochastic process is introduced, and it is shown that population distance covariance coincides with the covariance with respect to Brownian motion; thus, both can be called Brownian distance covariance. In the bivariate case, Brownian covariance is the natural extension of product-moment covariance, as we obtain Pearson product-moment covariance by replacing the Brownian motion in the definition with identity. The corresponding statistic has an elegantly simple computing formula. Advantages of applying Brownian covariance and correlation vs the classical Pearson covariance and correlation are discussed and illustrated.},
	pages = {1236--1265},
	number = {4},
	journaltitle = {The Annals of Applied Statistics},
	shortjournal = {Ann. Appl. Stat.},
	author = {Székely, Gábor J. and Rizzo, Maria L.},
	urldate = {2019-04-18},
	date = {2009-12},
	mrnumber = {MR2752127},
	zmnumber = {1196.62077},
	keywords = {Brownian covariance, dcor, Distance correlation, independence, multivariate},
	file = {Full Text PDF:/Users/adamelder/Zotero/storage/PS2GF2LG/Székely and Rizzo - 2009 - Brownian distance covariance.pdf:application/pdf;Snapshot:/Users/adamelder/Zotero/storage/9D9ICQ2A/1267453933.html:text/html}
}

@article{zhang_comment_2015,
	title = {Comment},
	volume = {110},
	issn = {0162-1459},
	url = {https://amstat.tandfonline.com/doi/full/10.1080/01621459.2015.1106403},
	doi = {10.1080/01621459.2015.1106403},
	pages = {1451--1454},
	number = {512},
	journaltitle = {Journal of the American Statistical Association},
	shortjournal = {Journal of the American Statistical Association},
	author = {Zhang, Yichi and Laber, Eric B.},
	urldate = {2019-04-18},
	date = {2015-10-02},
	file = {Snapshot:/Users/adamelder/Zotero/storage/QM9KTJIV/01621459.2015.html:text/html}
}

@article{hochberg_sharper_1988,
	title = {A sharper Bonferroni procedure for multiple tests of significance},
	volume = {75},
	issn = {0006-3444},
	url = {https://academic.oup.com/biomet/article/75/4/800/423177},
	doi = {10.1093/biomet/75.4.800},
	abstract = {Abstract.  A simple procedure for multiple tests of significance based on individual p-values is derived. This simple procedure is sharper than Holm's (1979) se},
	pages = {800--802},
	number = {4},
	journaltitle = {Biometrika},
	shortjournal = {Biometrika},
	author = {Hochberg, Yosef},
	urldate = {2019-04-18},
	date = {1988-12-01},
	langid = {english},
	file = {Snapshot:/Users/adamelder/Zotero/storage/H8WZ9YLC/423177.html:text/html}
}

@article{benjamini_controlling_1995,
	title = {Controlling the False Discovery Rate: A Practical and Powerful Approach to Multiple Testing},
	volume = {57},
	rights = {© 1995 Royal Statistical Society},
	issn = {2517-6161},
	url = {https://rss.onlinelibrary.wiley.com/doi/abs/10.1111/j.2517-6161.1995.tb02031.x},
	doi = {10.1111/j.2517-6161.1995.tb02031.x},
	shorttitle = {Controlling the False Discovery Rate},
	abstract = {The common approach to the multiplicity problem calls for controlling the familywise error rate ({FWER}). This approach, though, has faults, and we point out a few. A different approach to problems of multiple significance testing is presented. It calls for controlling the expected proportion of falsely rejected hypotheses — the false discovery rate. This error rate is equivalent to the {FWER} when all hypotheses are true but is smaller otherwise. Therefore, in problems where the control of the false discovery rate rather than that of the {FWER} is desired, there is potential for a gain in power. A simple sequential Bonferronitype procedure is proved to control the false discovery rate for independent test statistics, and a simulation study shows that the gain in power is substantial. The use of the new procedure and the appropriateness of the criterion are illustrated with examples.},
	pages = {289--300},
	number = {1},
	journaltitle = {Journal of the Royal Statistical Society: Series B (Methodological)},
	author = {Benjamini, Yoav and Hochberg, Yosef},
	urldate = {2019-04-18},
	date = {1995},
	langid = {english},
	keywords = {bonferroni-type procedures, familywise error rate, multiple-comparison procedures, p-values},
	file = {Full Text PDF:/Users/adamelder/Zotero/storage/AWM5MF4X/Benjamini and Hochberg - 1995 - Controlling the False Discovery Rate A Practical .pdf:application/pdf;Snapshot:/Users/adamelder/Zotero/storage/KIFIL6F8/j.2517-6161.1995.tb02031.html:text/html}
}

@article{holm_simple_1979,
	title = {A Simple Sequentially Rejective Multiple Test Procedure},
	volume = {6},
	doi = {10.2307/4615733},
	abstract = {This paper presents a simple and widely ap- plicable multiple test procedure of the sequentially rejective type, i.e. hypotheses are rejected one at a tine until no further rejections can be done. It is shown that the test has a prescribed level of significance protection against error of the first kind for any combination of true hypotheses. The power properties of the test and a number of possible applications are also discussed.},
	pages = {65--70},
	journaltitle = {Scandinavian Journal of Statistics},
	shortjournal = {Scandinavian Journal of Statistics},
	author = {Holm, Sture},
	date = {1979-01-01}
}

@article{neyman_jerzy_ix._1933,
	title = {{IX}. On the problem of the most efficient tests of statistical hypotheses},
	volume = {231},
	url = {https://royalsocietypublishing.org/doi/abs/10.1098/rsta.1933.0009},
	doi = {10.1098/rsta.1933.0009},
	abstract = {The problem of testing statistical hypotheses is an old one. Its origin is usually connected with the name of Thomas Bayes, who gave the well-known theorem on the probabilities a posteriori of the possible “causes" of a given event. Since then it has been discussed by many writers of whom we shall here mention two only, Bertrand and Borel, whose differing views serve well to illustrate the point from which we shall approach the subject. Bertrand put into statistical form a variety of hypotheses, as for example the hypothesis that a given group of stars with relatively small angular distances between them as seen from the earth, form a “system” or group in space. His method of attack, which is that in common use, consisted essentially in calculating the probability, P, that a certain character, x, of the observed facts would arise if the hypothesis tested were true. If P were very small, this would generally be considered as an indication that the hypothesis, H, was probably false, and vice versa. Bertrand expressed the pessimistic view that no test of this kind could give reliable results. Borel, however, in a later discussion, considered that the method described could be applied with success provided that the character, x, of the observed facts were properly chosen—were, in fact, a character which he terms “en quelque sorte remarquable.”},
	pages = {289--337},
	number = {694},
	journaltitle = {Philosophical Transactions of the Royal Society of London. Series A, Containing Papers of a Mathematical or Physical Character},
	shortjournal = {Philosophical Transactions of the Royal Society of London. Series A, Containing Papers of a Mathematical or Physical Character},
	author = {{Neyman Jerzy} and {Pearson Egon Sharpe} and {Pearson Karl}},
	urldate = {2019-05-13},
	date = {1933-02-16},
	file = {Full Text PDF:/Users/adamelder/Zotero/storage/VKN92DAL/Neyman Jerzy et al. - 1933 - IX. On the problem of the most efficient tests of .pdf:application/pdf;Snapshot:/Users/adamelder/Zotero/storage/TKPULF83/rsta.1933.html:text/html}
}

@article{dunn_multiple_1961,
	title = {Multiple Comparisons among Means},
	volume = {56},
	issn = {0162-1459},
	url = {https://www.tandfonline.com/doi/abs/10.1080/01621459.1961.10482090},
	doi = {10.1080/01621459.1961.10482090},
	abstract = {Methods for constructing simultaneous confidence intervals for all possible linear contrasts among several means of normally distributed variables have been given by Scheffé and Tukey. In this paper the possibility is considered of picking in advance a number (say m) of linear contrasts among k means, and then estimating these m linear contrasts by confidence intervals based on a Student t statistic, in such a way that the overall confidence level for the m intervals is greater than or equal to a preassigned value. It is found that for some values of k, and for m not too large, intervals obtained in this way are shorter than those using the F distribution or the Studentized range. When this is so, the experimenter may be willing to select the linear combinations in advance which he wishes to estimate in order to have m shorter intervals instead of an infinite number of longer intervals.},
	pages = {52--64},
	number = {293},
	journaltitle = {Journal of the American Statistical Association},
	author = {Dunn, Olive Jean},
	urldate = {2019-05-17},
	date = {1961-03-01},
	file = {Snapshot:/Users/adamelder/Zotero/storage/J7Q9S7JP/01621459.1961.html:text/html;Submitted Version:/Users/adamelder/Zotero/storage/M2EZRCNE/Dunn - 1961 - Multiple Comparisons among Means.pdf:application/pdf}
}

@article{dunn_estimation_1959,
	title = {Estimation of the Medians for Dependent Variables},
	volume = {30},
	issn = {0003-4851, 2168-8990},
	url = {https://projecteuclid.org/euclid.aoms/1177706374},
	doi = {10.1214/aoms/1177706374},
	abstract = {Joint intervals of bounded confidence are suggested for the medians of a bivariate population with continuous marginal distributions. The two intervals are of the classic type based on sample order statistics.},
	pages = {192--197},
	number = {1},
	journaltitle = {The Annals of Mathematical Statistics},
	shortjournal = {Ann. Math. Statist.},
	author = {Dunn, Olive Jean},
	urldate = {2019-05-17},
	date = {1959-03},
	mrnumber = {MR103563},
	zmnumber = {0090.36404},
	file = {Full Text PDF:/Users/adamelder/Zotero/storage/4598I8GF/Dunn - 1959 - Estimation of the Medians for Dependent Variables.pdf:application/pdf;Snapshot:/Users/adamelder/Zotero/storage/ZVQ62UQ7/1177706374.html:text/html}
}

@book{miller_simultaneous_1981,
	location = {New York},
	edition = {2},
	title = {Simultaneous Statistical Inference},
	isbn = {978-1-4613-8124-2},
	url = {https://www.springer.com/la/book/9781461381242},
	series = {Springer Series in Statistics},
	abstract = {Simultaneous Statistical Inference, which was published originally in 1966 by {McGraw}-Hill Book Company, went out of print in 1973. Since then, it has been available from University Microfilms International in xerox form. With this new edition Springer-Verlag has republished the original edition along with my review article on multiple comparisons from the December 1977 issue of the Journal of the American Statistical Association. This review article covered developments in the field from 1966 through 1976. A few minor typographical errors in the original edition have been corrected in this new edition. A new table of critical points for the studentized maximum modulus is included in this second edition as an addendum. The original edition included the table by K. C. S. Pillai and K. V. Ramachandran, which was meager but the best available at the time. This edition contains the table published in Biometrika in 1971 by G. 1. Hahn and R. W. Hendrickson, which is far more comprehensive and therefore more useful. The typing was ably handled by Wanda Edminster for the review article and Karola Decleve for the changes for the second edition. My wife, Barbara, again cheerfully assisted in the proofreading. Fred Leone kindly granted permission from the American Statistical Association to reproduce my review article. Also, Gerald Hahn, Richard Hendrickson, and, for Biometrika, David Cox graciously granted permission to reproduce the new table of the studentized maximum modulus. The work in preparing the review article was partially supported by {NIH} Grant {ROI} {GM}21215.},
	publisher = {Springer-Verlag},
	author = {Miller, Rupert G. Jr},
	urldate = {2019-06-06},
	date = {1981},
	langid = {english},
	file = {Snapshot:/Users/adamelder/Zotero/storage/PM7WRB22/9781461381242.html:text/html}
}

@article{pan_powerful_2014,
	title = {A Powerful and Adaptive Association Test for Rare Variants},
	volume = {197},
	rights = {Copyright © 2014 by the Genetics Society of America},
	issn = {0016-6731, 1943-2631},
	url = {https://www.genetics.org/content/197/4/1081},
	doi = {10.1534/genetics.114.165035},
	abstract = {This article focuses on conducting global testing for association between a binary trait and a set of rare variants ({RVs}), although its application can be much broader to other types of traits, common variants ({CVs}), and gene set or pathway analysis. We show that many of the existing tests have deteriorating performance in the presence of many nonassociated {RVs}: their power can dramatically drop as the proportion of nonassociated {RVs} in the group to be tested increases. We propose a class of so-called sum of powered score ({SPU}) tests, each of which is based on the score vector from a general regression model and hence can deal with different types of traits and adjust for covariates, e.g., principal components accounting for population stratification. The {SPU} tests generalize the sum test, a representative burden test based on pooling or collapsing genotypes of {RVs}, and a sum of squared score ({SSU}) test that is closely related to several other powerful variance component tests; a previous study (Basu and Pan 2011) has demonstrated good performance of one, but not both, of the Sum and {SSU} tests in many situations. The {SPU} tests are versatile in the sense that one of them is often powerful, although its identity varies with the unknown true association parameters. We propose an adaptive {SPU} ({aSPU}) test to approximate the most powerful {SPU} test for a given scenario, consequently maintaining high power and being highly adaptive across various scenarios. We conducted extensive simulations to show superior performance of the {aSPU} test over several state-of-the-art association tests in the presence of many nonassociated {RVs}. Finally we applied the {SPU} and {aSPU} tests to the {GAW}17 mini-exome sequence data to compare its practical performance with some existing tests, demonstrating their potential usefulness.},
	pages = {1081--1095},
	number = {4},
	journaltitle = {Genetics},
	author = {Pan, Wei and Kim, Junghi and Zhang, Yiwei and Shen, Xiaotong and Wei, Peng},
	urldate = {2019-07-12},
	date = {2014-08-01},
	langid = {english},
	pmid = {24831820},
	keywords = {adaptive {SPU} ({aSPU}) test, genome-wide association study ({GWAS}), score statistic, sequencing data, sum of powered score ({SPU}) test, sum of squared score ({SSU}) test, sum test},
	file = {Full Text PDF:/Users/adamelder/Zotero/storage/7LQUE2Z4/Pan et al. - 2014 - A Powerful and Adaptive Association Test for Rare .pdf:application/pdf;Snapshot:/Users/adamelder/Zotero/storage/MF2EXY8V/1081.html:text/html}
}

@article{xu_adaptive_2016,
	title = {An adaptive two-sample test for high-dimensional means},
	volume = {103},
	issn = {0006-3444},
	url = {https://academic.oup.com/biomet/article/103/3/609/1744173},
	doi = {10.1093/biomet/asw029},
	abstract = {{SUMMARY}.  Several two-sample tests for high-dimensional data have been proposed recently, but they are powerful only against certain alternative hypotheses. In},
	pages = {609--624},
	number = {3},
	journaltitle = {Biometrika},
	shortjournal = {Biometrika},
	author = {Xu, Gongjun and Lin, Lifeng and Wei, Peng and Pan, Wei},
	urldate = {2019-07-12},
	date = {2016-09-01},
	langid = {english},
	file = {Full Text PDF:/Users/adamelder/Zotero/storage/75SAE8HG/Xu et al. - 2016 - An adaptive two-sample test for high-dimensional m.pdf:application/pdf;Snapshot:/Users/adamelder/Zotero/storage/XXISDRKN/1744173.html:text/html}
}

@article{fan_sure_2006,
	title = {Sure independence screening for ultra-high dimensional feature space},
	abstract = {Variable selection plays an important role in high dimensional statistical modeling which nowa-days appears in many areas and is key to various scientific discoveries. For problems of large scale or dimensionality p, estimation accuracy and computational cost are two top concerns. In a recent paper, Candes and Tao (2007) propose the Dantzig selector using L1 regularization and show that it achieves the ideal risk up to a logarithmic factor log p. Their innovative procedure and remarkable result are challenged when the dimensionality is ultra high as the factor log p can be large and their uniform uncertainty principle can fail. Motivated by these concerns, we introduce the concept of sure screening and propose a sure screening method based on a correlation learning, called the Sure Independence Screening ({SIS}), to reduce dimensionality from high to a moderate scale that is below sample size. In a fairly general asymptotic framework, the {SIS} is shown to have the sure screening property for even exponentially growing dimensionality. As a methodological extension, an iterative {SIS} ({ISIS}) is also proposed to enhance its finite sample performance. With dimension reduced accurately from high to below sample size, variable selection can be improved on both speed and accuracy, and can then be ac-},
	journaltitle = {Journal of the Royal Statistical Society, Series B},
	author = {Fan, Jianqing and Lv, Jinchi},
	date = {2006},
	file = {Citeseer - Full Text PDF:/Users/adamelder/Zotero/storage/2H5KE8ST/Fan and Lv - 2006 - Sure independence screening for ultra-high dimensi.pdf:application/pdf;Citeseer - Snapshot:/Users/adamelder/Zotero/storage/J94YY96Z/summary.html:text/html}
}

@article{huang_adaptive_2006,
	title = {Adaptive {LASSO} for sparse high-dimensional regression},
	volume = {18},
	abstract = {We study the asymptotic properties of the adaptive Lasso estimators in sparse, high-dimensional, linear regression models when the number of covariates may increase with the sample size. We consider variable selection using the adaptive Lasso, where the L 1 norms in the penalty are re-weighted by data-dependent weights. We show that, if a reasonable initial estimator is available, under appropriate conditions, the adaptive Lasso correctly selects covariates with nonzero coefficients with probability converging to one, and that the estimators of nonzero coefficients have the same asymptotic distribution they would have if the zero coefficients were known in advance. Thus, the adaptive Lasso has an oracle property in the sense of J. Fan and R. Li [J. Am. Stat. Assoc. 96, No. 456, 1348–1360 (2001; Zbl 1073.62547)] and J. Fan and H. Peng [Ann. Stat. 32, No. 3, 928–961 (2004; Zbl 1092.62031)]. In addition, under a partial orthogonality condition in which the covariates with zero coefficients are weakly correlated with the covariates with nonzero coefficients, marginal regression can be used to obtain the initial estimator. With this initial estimator, the adaptive Lasso has the oracle property even when the number of covariates is much larger than the sample size.},
	journaltitle = {Statistica Sinica},
	shortjournal = {Statistica Sinica},
	author = {Huang, Jian and Ma, Shuangge and Zhang, Cun-Hui},
	date = {2006-12-01},
	file = {Full Text PDF:/Users/adamelder/Zotero/storage/VKBEI36K/Huang et al. - 2006 - Adaptive LASSO for sparse high-dimensional regress.pdf:application/pdf}
}

@article{anderson_permutation_2001,
	title = {Permutation Tests for Linear Models},
	volume = {43},
	rights = {Australian Statistical Publishing Association Inc. 2001},
	issn = {1467-842X},
	url = {https://onlinelibrary.wiley.com/doi/abs/10.1111/1467-842X.00156},
	doi = {10.1111/1467-842X.00156},
	abstract = {Several approximate permutation tests have been proposed for tests of partial regression coefficients in a linear model based on sample partial correlations. This paper begins with an explanation and notation for an exact test. It then compares the distributions of the test statistics under the various permutation methods proposed, and shows that the partial correlations under permutation are asymptotically jointly normal with means 0 and variances 1. The method of Freedman \& Lane (1983) is found to have asymptotic correlation 1 with the exact test, and the other methods are found to have smaller correlations with this test. Under local alternatives the critical values of all the approximate permutation tests converge to the same constant, so they all have the same asymptotic power. Simulations demonstrate these theoretical results.},
	pages = {75--88},
	number = {1},
	journaltitle = {Australian \& New Zealand Journal of Statistics},
	author = {Anderson, Marti J. and Robinson, John},
	urldate = {2019-07-30},
	date = {2001},
	langid = {english},
	keywords = {asymptotics, partial correlations, power, resampling.},
	file = {Full Text PDF:/Users/adamelder/Zotero/storage/T9AG5LBD/Anderson and Robinson - 2001 - Permutation Tests for Linear Models.pdf:application/pdf;Snapshot:/Users/adamelder/Zotero/storage/L4ZIB8CQ/1467-842X.html:text/html}
}

@article{omelka_testing_2012,
	title = {Testing equality of correlation coefficients in two populations via permutation methods},
	volume = {142},
	issn = {0378-3758},
	url = {http://www.sciencedirect.com/science/article/pii/S0378375811004630},
	doi = {10.1016/j.jspi.2011.12.018},
	abstract = {The present paper investigates the asymptotic behaviour of a studentized permutation test for testing equality of (Pearson) correlation coefficients in two populations. It is shown that this test is asymptotically of exact level and has the same power for contiguous alternatives as the corresponding asymptotic test. As a by-product we specify the assumptions needed for the validity of the permutation test suggested in Sakaori (2002). A small simulation study compares the finite sample properties of the considered tests.},
	pages = {1396--1406},
	number = {6},
	journaltitle = {Journal of Statistical Planning and Inference},
	shortjournal = {Journal of Statistical Planning and Inference},
	author = {Omelka, M. and Pauly, M.},
	urldate = {2019-07-30},
	date = {2012-06-01},
	keywords = {Behrens–Fisher problem, Contiguous alternatives, Pearson correlation coefficient, Permutation tests, Power of a test, Studentized statistics, Two-sample tests},
	file = {ScienceDirect Full Text PDF:/Users/adamelder/Zotero/storage/HELQ8FVS/Omelka and Pauly - 2012 - Testing equality of correlation coefficients in tw.pdf:application/pdf;ScienceDirect Snapshot:/Users/adamelder/Zotero/storage/XBQZU8F7/S0378375811004630.html:text/html}
}

@inproceedings{gupta_inequalitites_1972,
	title = {Inequalitites on the probability content of convex regions for elliptically contoured distributions},
	url = {https://projecteuclid.org/euclid.bsmsp/1200514222},
	abstract = {Project Euclid - mathematics and statistics online},
	eventtitle = {Proceedings of the Sixth Berkeley Symposium on Mathematical Statistics and Probability, Volume 2: Probability Theory},
	publisher = {The Regents of the University of California},
	author = {Gupta, S. Das and Eaton, M. L. and Olkin, I. and Perlman, M. and Savage, L. J. and Sobel, M.},
	urldate = {2019-07-30},
	date = {1972},
	file = {Full Text PDF:/Users/adamelder/Zotero/storage/HEXYGS5F/Gupta et al. - 1972 - Inequalitites on the probability content of convex.pdf:application/pdf;Snapshot:/Users/adamelder/Zotero/storage/GA9LP8RL/1200514222.html:text/html}
}

@article{eaton_concentration_1991,
	title = {Concentration inequalities for multivariate distributions: I. multivariate normal distributions},
	volume = {12},
	issn = {0167-7152},
	url = {http://www.sciencedirect.com/science/article/pii/016771529190004B},
	doi = {10.1016/0167-7152(91)90004-B},
	shorttitle = {Concentration inequalities for multivariate distributions},
	abstract = {Let X ∼ Np(0, Σ), the p-variate normal distribution with mean 0 and positive definite covariance matrix Σ. Anderson (1955) showed that if Σ2 − Σ1 is positive semidefinite then {PΣ}1(C) ⩾ {PΣ}2(C) for every centrally symmetric (− C = C) convex set C⊆Rp. Fefferman, Jodeit and Perlman (1972) extended this result to elliptically contoured distributions. In the present study similar multivariate concentration inequalities are investigated for convex sets C that satisfy a more general symmetry condition, namely invariance under a group G of orthogonal transformations on Rp, as well as for non-convex sets C that are monotonically decreasing with respect to a pre-ordering determined by G. Both new results and counterexamples are presented. Concentration inequalities may be used to convert classical efficiency comparisons, expressed in terms of covariance matrices, into comparisons of probabilities of multivariate regions.},
	pages = {487--504},
	number = {6},
	journaltitle = {Statistics \& Probability Letters},
	shortjournal = {Statistics \& Probability Letters},
	author = {Eaton, Morris L. and Perlman, Michael D.},
	urldate = {2020-01-08},
	date = {1991-12-01},
	langid = {english},
	file = {ScienceDirect Snapshot:/Users/adamelder/Zotero/storage/YVG6KRE5/016771529190004B.html:text/html}
}

@article{anderson_integral_1955,
	title = {The Integral of a Symmetric Unimodal Function over a Symmetric Convex Set and Some Probability Inequalities},
	volume = {6},
	issn = {0002-9939},
	url = {https://www.jstor.org/stable/2032333},
	doi = {10.2307/2032333},
	pages = {170--176},
	number = {2},
	journaltitle = {Proceedings of the American Mathematical Society},
	author = {Anderson, T. W.},
	urldate = {2020-01-09},
	date = {1955}
}

@article{pinelis_schur2-concavity_2014,
	title = {Schur2-concavity properties of Gaussian measures, with applications to hypotheses testing},
	volume = {124},
	issn = {0047-259X},
	url = {http://www.sciencedirect.com/science/article/pii/S0047259X13002534},
	doi = {10.1016/j.jmva.2013.11.011},
	abstract = {The main results imply that the probability P(Z∈A+θ) is Schur-concave/Schur-convex in (θ12,…,θk2) provided that the indicator function of a set A in Rk is so, respectively; here, θ=(θ1,…,θk)∈Rk and Z is a standard normal random vector in Rk. Moreover, it is shown that the Schur-concavity/Schur-convexity is strict unless the set A is equivalent to a spherically symmetric set. Applications to testing hypotheses on multivariate means are given.},
	pages = {384--397},
	journaltitle = {Journal of Multivariate Analysis},
	shortjournal = {Journal of Multivariate Analysis},
	author = {Pinelis, Iosif},
	urldate = {2020-02-06},
	date = {2014-02-01},
	langid = {english},
	keywords = {-mean tests, Asymptotic properties of tests, Asymptotic relative efficiency, Gaussian measures, Geometric probability, Hypothesis testing, Majorization, Mixtures, Multivariate means, Multivariate normal distribution, Probability inequalities, Reflection groups, Schur convexity, Stochastic ordering},
	file = {ScienceDirect Full Text PDF:/Users/adamelder/Zotero/storage/YIWRF3TI/Pinelis - 2014 - Schur2-concavity properties of Gaussian measures, .pdf:application/pdf;ScienceDirect Snapshot:/Users/adamelder/Zotero/storage/Z77MJBKH/S0047259X13002534.html:text/html}
}

@book{van_der_vaart_asymptotic_2000,
	location = {Cambridge},
	title = {Asymptotic Statistics},
	isbn = {978-0-521-78450-4},
	pagetotal = {460},
	publisher = {Cambridge University Press},
	author = {Van der Vaart, A. W.},
	date = {2000-06-19}
}

@article{rinott_convexity_1976,
	title = {On Convexity of Measures},
	volume = {4},
	issn = {0091-1798},
	url = {https://www.jstor.org/stable/2242963},
	abstract = {A simple geometric proof and some applications are given to results of C. Borell providing necessary and sufficient conditions that a density in \$R{\textasciicircum}n\$ generates a measure satisfying a convexity property of the type \$\$P({\textbackslash}theta A\_0 + (1 - {\textbackslash}theta)A\_1) {\textbackslash}geqq {\textbackslash}\{{\textbackslash}theta{\textbackslash}lbrack P(A\_0){\textbackslash}rbrack{\textasciicircum}s + (1 - {\textbackslash}theta){\textbackslash}lbrack P(A\_1) {\textbackslash}rbrack{\textasciicircum}s{\textbackslash}\}{\textasciicircum}\{1/s\}.\$\$},
	pages = {1020--1026},
	number = {6},
	journaltitle = {The Annals of Probability},
	author = {Rinott, Yosef},
	urldate = {2020-04-10},
	date = {1976},
	note = {Publisher: Institute of Mathematical Statistics},
	file = {JSTOR Full Text PDF:/Users/adamelder/Zotero/storage/2PT996TH/Rinott - 1976 - On Convexity of Measures.pdf:application/pdf}
}

@book{tong_multivariate_2012,
	title = {The Multivariate Normal Distribution},
	isbn = {978-1-4613-9655-0},
	abstract = {The multivariate normal distribution has played a predominant role in the historical development of statistical theory, and has made its appearance in various areas of applications. Although many of the results concerning the multivariate normal distribution are classical, there are important new results which have been reported recently in the literature but cannot be found in most books on multivariate analysis. These results are often obtained by showing that the multivariate normal density function belongs to certain large families of density functions. Thus, useful properties of such families immedi ately hold for the multivariate normal distribution. This book attempts to provide a comprehensive and coherent treatment of the classical and new results related to the multivariate normal distribution. The material is organized in a unified modern approach, and the main themes are dependence, probability inequalities, and their roles in theory and applica tions. Some general properties of a multivariate normal density function are discussed, and results that follow from these properties are reviewed exten sively. The coverage is, to some extent, a matter of taste and is not intended to be exhaustive, thus more attention is focused on a systematic presentation of results rather than on a complete listing of them.},
	pagetotal = {281},
	publisher = {Springer Science \& Business Media},
	author = {Tong, Y. L.},
	date = {2012-12-06},
	langid = {english},
	note = {Google-Books-{ID}: {FtHgBwAAQBAJ}},
	keywords = {Business \& Economics / Economics / General, Business \& Economics / Economics / Theory}
}

@article{eaton_multivariate_1991,
	title = {Multivariate probability inequalities: Convolution theorems, composition theorems, and concentration inequalities},
	volume = {19},
	url = {https://zbmath.org/?q=an%3A0760.60017},
	shorttitle = {Multivariate probability inequalities},
	abstract = {[For the entire collection see Zbl 0745.00058.]
Several important multivariate probability inequalities can be formulated in terms of multivariate convolutions of the form ∫f1(x)f2(x−θ)dx∫f1(x)f2(x−θ)dx{\textbackslash}int f\_ 1(x)f\_ 2(x-{\textbackslash}theta)dx, where usually f1={ICf}1={ICf}\_ 1=I\_ C is the indicator of a region C⊆ℝ{nC}⊆{RnC}{\textbackslash}subseteq{\textbackslash}mathbb\{R\}{\textasciicircum} n, f2f2f\_ 2 is a probability density on ℝ{nRn}{\textbackslash}mathbb\{R\}{\textasciicircum} n, and θθ{\textbackslash}theta is a translation parameter. Often f1f1f\_ 1 and f2f2f\_ 2 possess convexity, monotonicity, and/or symmetry properties. More general multivariate compositions of the form ∫h(x)f(x∣θ)μ(dx)∫h(x)f(x∣θ)μ(dx){\textbackslash}int h(x)f(x{\textbackslash}mid{\textbackslash}theta){\textbackslash}mu(dx) also arise. Here several important convolution and composition theorems will be reviewed; these provide comparisons of Prob(C)Prob(C){\textbackslash}text\{Prob\}(C) under differing multivariate distributions. The convolution theorems are then applied to obtain concentration inequalities for Prob(C)Prob(C){\textbackslash}text\{Prob\}(C) under Gaussian or elliptically contoured distributions with varying multivariate scale parameter ΣΣ{\textbackslash}Sigma.},
	pages = {104--122},
	journaltitle = {Institute of Mathematical Statistics Lecture Notes - Monograph Series},
	author = {Eaton, Morris L. and Perlman, Michael D.},
	urldate = {2020-05-04},
	date = {1991},
	zmnumber = {0760.60017},
	note = {{MSC}2010: 
                                                60E15
                                             = 
                                                Inequalities; stochastic orderings
                                            
{MSC}2010: 
                                                62H10
                                             = 
                                                Multivariate distribution of statistics
                                            },
	keywords = {concentration inequalities, convexity, convolution theorems, multivariate convolutions, multivariate probability inequalities, symmetry properties},
	file = {Snapshot:/Users/adamelder/Zotero/storage/V7EHK2SM/zbmath.org.html:text/html}
}

@article{friedman_regularization_2010,
	title = {Regularization Paths for Generalized Linear Models via Coordinate Descent},
	volume = {33},
	issn = {1548-7660},
	url = {https://www.ncbi.nlm.nih.gov/pmc/articles/PMC2929880/},
	abstract = {We develop fast algorithms for estimation of generalized linear models with convex penalties. The models include linear regression, two-class logistic regression, and multinomial regression problems while the penalties include ℓ1 (the lasso), ℓ2 (ridge regression) and mixtures of the two (the elastic net). The algorithms use cyclical coordinate descent, computed along a regularization path. The methods can handle large problems and can also deal efficiently with sparse features. In comparative timings we find that the new algorithms are considerably faster than competing methods.},
	pages = {1--22},
	number = {1},
	journaltitle = {Journal of statistical software},
	shortjournal = {J Stat Softw},
	author = {Friedman, Jerome and Hastie, Trevor and Tibshirani, Rob},
	urldate = {2020-05-04},
	date = {2010},
	pmid = {20808728},
	pmcid = {PMC2929880},
	file = {PubMed Central Full Text PDF:/Users/adamelder/Zotero/storage/SAG5CC4H/Friedman et al. - 2010 - Regularization Paths for Generalized Linear Models.pdf:application/pdf}
}

@article{simon_blockwise_2013,
	title = {A Blockwise Descent Algorithm for Group-penalized Multiresponse and Multinomial Regression},
	url = {http://arxiv.org/abs/1311.6529},
	abstract = {In this paper we purpose a blockwise descent algorithm for group-penalized multiresponse regression. Using a quasi-newton framework we extend this to group-penalized multinomial regression. We give a publicly available implementation for these in R, and compare the speed of this algorithm to a competing algorithm --- we show that our implementation is an order of magnitude faster than its competitor, and can solve gene-expression-sized problems in real time.},
	journaltitle = {{arXiv}:1311.6529 [stat]},
	author = {Simon, Noah and Friedman, Jerome and Hastie, Trevor},
	urldate = {2020-05-04},
	date = {2013-11-25},
	eprinttype = {arxiv},
	eprint = {1311.6529},
	keywords = {Statistics - Computation, Statistics - Machine Learning},
	file = {arXiv Fulltext PDF:/Users/adamelder/Zotero/storage/2QBMNDWB/Simon et al. - 2013 - A Blockwise Descent Algorithm for Group-penalized .pdf:application/pdf;arXiv.org Snapshot:/Users/adamelder/Zotero/storage/ZKVHY39B/1311.html:text/html}
}

@article{tibshirani_strong_2012,
	title = {Strong rules for discarding predictors in lasso-type problems},
	volume = {74},
	issn = {1369-7412},
	url = {https://www.ncbi.nlm.nih.gov/pmc/articles/PMC4262615/},
	doi = {10.1111/j.1467-9868.2011.01004.x},
	abstract = {We consider rules for discarding predictors in lasso regression and related problems, for computational efficiency. El Ghaoui and his colleagues have propose ‘{SAFE}’ rules, based on univariate inner products between each predictor and the outcome, which guarantee that a coefficient will be 0 in the solution vector. This provides a reduction in the number of variables that need to be entered into the optimization. We propose strong rules that are very simple and yet screen out far more predictors than the {SAFE} rules. This great practical improvement comes at a price: the strong rules are not foolproof and can mistakenly discard active predictors, i.e. predictors that have non-zero coefficients in the solution. We therefore combine them with simple checks of the Karush–Kuhn–Tucker conditions to ensure that the exact solution to the convex problem is delivered. Of course, any (approximate) screening method can be combined with the Karush–Kuhn–Tucker, conditions to ensure the exact solution; the strength of the strong rules lies in the fact that, in practice, they discard a very large number of the inactive predictors and almost never commit mistakes. We also derive conditions under which they are foolproof. Strong rules provide substantial savings in computational time for a variety of statistical optimization problems.},
	pages = {245--266},
	number = {2},
	journaltitle = {Journal of the Royal Statistical Society. Series B, Statistical methodology},
	shortjournal = {J R Stat Soc Series B Stat Methodol},
	author = {Tibshirani, Robert and Bien, Jacob and Friedman, Jerome and Hastie, Trevor and Simon, Noah and Taylor, Jonathan and Tibshirani, Ryan J.},
	urldate = {2020-05-04},
	date = {2012-03},
	pmid = {25506256},
	pmcid = {PMC4262615},
	file = {PubMed Central Full Text PDF:/Users/adamelder/Zotero/storage/V3SBFQ6H/Tibshirani et al. - 2012 - Strong rules for discarding predictors in lasso-ty.pdf:application/pdf}
}

@article{polley_super_2010,
	title = {Super Learner In Prediction},
	url = {https://biostats.bepress.com/ucbbiostat/paper266},
	journaltitle = {U.C. Berkeley Division of Biostatistics Working Paper Series},
	author = {Polley, Eric and Laan, Mark van der},
	date = {2010-05-03},
	file = {"Super Learner In Prediction" by Eric C. Polley and Mark J. van der Laan:/Users/adamelder/Zotero/storage/8PRFCXB7/paper266.html:text/html}
}

@article{lendle_ltmle_2017,
	title = {ltmle: An R Package Implementing Targeted Minimum Loss-Based Estimation for Longitudinal Data},
	volume = {81},
	rights = {Copyright (c) 2017 Samuel D. Lendle, Joshua Schwab, Maya L. Petersen, Mark J. van der Laan},
	issn = {1548-7660},
	url = {https://www.jstatsoft.org/index.php/jss/article/view/v081i01},
	doi = {10.18637/jss.v081.i01},
	shorttitle = {ltmle},
	pages = {1--21},
	number = {1},
	journaltitle = {Journal of Statistical Software},
	author = {Lendle, Samuel D. and Schwab, Joshua and Petersen, Maya L. and Laan, Mark J. van der},
	urldate = {2020-05-04},
	date = {2017-10-16},
	langid = {english},
	note = {Number: 1},
	keywords = {causal inference, estimation, longitudinal data, R, targeted minimum loss-based estimation},
	file = {Full Text:/Users/adamelder/Zotero/storage/6FEMM8Y6/Lendle et al. - 2017 - ltmle An R Package Implementing Targeted Minimum .pdf:application/pdf;Snapshot:/Users/adamelder/Zotero/storage/QCPCLFJ9/v081i01.html:text/html}
}